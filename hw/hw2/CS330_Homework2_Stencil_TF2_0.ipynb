{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS330_Homework2_Stencil_TF2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkoC8rAYBE7"
      },
      "source": [
        "\n",
        "##Setup\n",
        "\n",
        "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_w8zVoQGb_y",
        "outputId": "045e73bf-4c37-41ba-fa37-26959a1e177e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkP5aBdfFkd",
        "outputId": "d002a851-a188-44c9-cac2-9927de7b8f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
        "if not os.path.isdir('./omniglot_resized'):\n",
        "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
        "                                        dest_path='./omniglot_resized.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "assert os.path.isdir('./omniglot_resized')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI into ./omniglot_resized.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtiYUiwI-1K"
      },
      "source": [
        "\"\"\" Utility functions. \"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "## Loss utilities\n",
        "def cross_entropy_loss(pred, label, k_shot):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_dTnU8JwWWc"
      },
      "source": [
        "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "seed = 123\n",
        "def conv_block(inp, cweight, bweight, bn, activation=tf.nn.relu, residual=False):\n",
        "  \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
        "  stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
        "\n",
        "  conv_output = tf.nn.conv2d(input=inp, filters=cweight, strides=no_stride, padding='SAME') + bweight\n",
        "  normed = bn(conv_output)\n",
        "  normed = activation(normed)\n",
        "  return normed\n",
        "\n",
        "class ConvLayers(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
        "    super(ConvLayers, self).__init__()\n",
        "    self.channels = channels\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.dim_output = dim_output\n",
        "    self.img_size = img_size\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    dtype = tf.float32\n",
        "    weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
        "    k = 3\n",
        "\n",
        "    weights['conv1'] = tf.Variable(weight_initializer(shape=[k, k, self.channels, self.dim_hidden]), name='conv1', dtype=dtype)\n",
        "    weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b1')\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization(name='bn1')\n",
        "    weights['conv2'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv2', dtype=dtype)\n",
        "    weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b2')\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization(name='bn2')\n",
        "    weights['conv3'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv3', dtype=dtype)\n",
        "    weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b3')\n",
        "    self.bn3 = tf.keras.layers.BatchNormalization(name='bn3')\n",
        "    weights['conv4'] = tf.Variable(weight_initializer([k, k, self.dim_hidden, self.dim_hidden]), name='conv4', dtype=dtype)\n",
        "    weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b4')\n",
        "    self.bn4 = tf.keras.layers.BatchNormalization(name='bn4')\n",
        "    weights['w5'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_output]), name='w5', dtype=dtype)\n",
        "    weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
        "    self.conv_weights = weights\n",
        "\n",
        "  def call(self, inp, weights):\n",
        "    channels = self.channels\n",
        "    inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
        "    hidden1 = conv_block(inp, weights['conv1'], weights['b1'], self.bn1)\n",
        "    hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], self.bn2)\n",
        "    hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], self.bn3)\n",
        "    hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], self.bn4)\n",
        "    hidden4 = tf.reduce_mean(input_tensor=hidden4, axis=[1, 2])\n",
        "    return tf.matmul(hidden4, weights['w5']) + weights['b5']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXZS_JULriBh"
      },
      "source": [
        "\"\"\"Data loading scripts\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "import imageio\n",
        "\n",
        "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "    paths: A list of character folders\n",
        "    labels: List or numpy array of same length as paths\n",
        "    n_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "    List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if n_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, n_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "           for i, path in zip(labels, paths)\n",
        "           for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "    filename: Image filename\n",
        "    dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "    1 channel image\n",
        "  \"\"\"\n",
        "  image = imageio.imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      num_classes: Number of classes for classification (K-way)\n",
        "      num_samples_per_class: num samples to generate per class in one batch\n",
        "      num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
        "      num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
        "      batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "    self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
        "    self.num_meta_test_classes = num_meta_test_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "               for family in os.listdir(data_folder)\n",
        "               if os.path.isdir(os.path.join(data_folder, family))\n",
        "               for character in os.listdir(os.path.join(data_folder, family))\n",
        "               if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(123)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "      num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "      batch_type: meta_train/meta_val/meta_test\n",
        "      shuffle: randomly shuffle classes or not\n",
        "      swap: swap number of classes (N) and number of samples per class (K) or not\n",
        "    Returns:\n",
        "      A a tuple of (1) Image batch and (2) Label batch where\n",
        "      image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
        "      where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"meta_train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    elif batch_type == \"meta_val\":\n",
        "      folders = self.metaval_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "      num_classes = self.num_meta_test_classes\n",
        "      num_samples_per_class = self.num_meta_test_samples_per_class\n",
        "    all_image_batches, all_label_batches = [], []\n",
        "    for i in range(batch_size):\n",
        "      sampled_character_folders = random.sample(\n",
        "        folders, num_classes)\n",
        "      labels_and_images = get_images(sampled_character_folders, range(\n",
        "        num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
        "      labels = [li[0] for li in labels_and_images]\n",
        "      images = [image_file_to_array(\n",
        "        li[1], self.dim_input) for li in labels_and_images]\n",
        "      images = np.stack(images)\n",
        "      labels = np.array(labels).astype(np.int32)\n",
        "      labels = np.reshape(\n",
        "        labels, (num_classes, num_samples_per_class))\n",
        "      labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
        "      images = np.reshape(\n",
        "        images, (num_classes, num_samples_per_class, -1))\n",
        "\n",
        "      batch = np.concatenate([labels, images], 2)\n",
        "      if shuffle:\n",
        "        for p in range(num_samples_per_class):\n",
        "          np.random.shuffle(batch[:, p])\n",
        "\n",
        "      labels = batch[:, :, :num_classes]\n",
        "      images = batch[:, :, num_classes:]\n",
        "\n",
        "      if swap:\n",
        "        labels = np.swapaxes(labels, 0, 1)\n",
        "        images = np.swapaxes(images, 0, 1)\n",
        "\n",
        "      all_image_batches.append(images)\n",
        "      all_label_batches.append(labels)\n",
        "    all_image_batches = np.stack(all_image_batches)\n",
        "    all_label_batches = np.stack(all_label_batches)\n",
        "    return all_image_batches, all_label_batches"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxriXFvwsGfp"
      },
      "source": [
        "\"\"\"MAML model code\"\"\"\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "  def __init__(self, dim_input=1, dim_output=1,\n",
        "               num_inner_updates=1,\n",
        "               inner_update_lr=0.4, num_filters=32, k_shot=5, learn_inner_update_lr=False):\n",
        "    super(MAML, self).__init__()\n",
        "    self.dim_input = dim_input\n",
        "    self.dim_output = dim_output\n",
        "    self.inner_update_lr = inner_update_lr\n",
        "    self.loss_func = partial(cross_entropy_loss, k_shot=k_shot)\n",
        "    self.dim_hidden = num_filters\n",
        "    self.channels = 1\n",
        "    self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
        "\n",
        "    # outputs_ts[i] and losses_ts_post[i] are the output and loss after i+1 inner gradient updates\n",
        "    losses_tr_pre, outputs_tr, losses_ts_post, outputs_ts = [], [], [], []\n",
        "    accuracies_tr_pre, accuracies_ts = [], []\n",
        "\n",
        "    # for each loop in the inner training loop\n",
        "    outputs_ts = [[]]*num_inner_updates\n",
        "    losses_ts_post = [[]]*num_inner_updates\n",
        "    accuracies_ts = [[]]*num_inner_updates\n",
        "\n",
        "    # Define the weights - these should NOT be directly modified by the\n",
        "    # inner training loop\n",
        "    tf.random.set_seed(seed)\n",
        "    self.conv_layers = ConvLayers(self.channels, self.dim_hidden, self.dim_output, self.img_size)\n",
        "\n",
        "    self.learn_inner_update_lr = learn_inner_update_lr\n",
        "    if self.learn_inner_update_lr:\n",
        "      self.inner_update_lr_dict = {}\n",
        "      for key in self.conv_layers.conv_weights.keys():\n",
        "        self.inner_update_lr_dict[key] = [tf.Variable(self.inner_update_lr, name='inner_update_lr_%s_%d' % (key, j)) for j in range(num_inner_updates)]\n",
        "  \n",
        "\n",
        "  def call(self, inp, meta_batch_size=25, num_inner_updates=1):\n",
        "    def task_inner_loop(inp, reuse=True,\n",
        "                      meta_batch_size=25, num_inner_updates=1):\n",
        "      \"\"\"\n",
        "        Perform gradient descent for one task in the meta-batch (i.e. inner-loop).\n",
        "        Args:\n",
        "          inp: a tuple (input_tr, input_ts, label_tr, label_ts), where input_tr and label_tr are the inputs and\n",
        "            labels used for calculating inner loop gradients and input_ts and label_ts are the inputs and\n",
        "            labels used for evaluating the model after inner updates.\n",
        "            Should be shapes:\n",
        "              input_tr: [N*K, 784]\n",
        "              input_ts: [N*K, 784]\n",
        "              label_tr: [N*K, N]\n",
        "              label_ts: [N*K, N]\n",
        "        Returns:\n",
        "          task_output: a list of outputs, losses and accuracies at each inner update\n",
        "      \"\"\"\n",
        "      # the inner and outer loop data\n",
        "      input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "      # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
        "      weights = self.conv_layers.conv_weights\n",
        "\n",
        "      # the predicted outputs, loss values, and accuracy for the pre-update model (with the initial weights)\n",
        "      # evaluated on the inner loop training data\n",
        "      task_output_tr_pre, task_loss_tr_pre, task_accuracy_tr_pre = None, None, None\n",
        "\n",
        "      # lists to keep track of outputs, losses, and accuracies of test data for each inner_update\n",
        "      # where task_outputs_ts[i], task_losses_ts[i], task_accuracies_ts[i] are the output, loss, and accuracy\n",
        "      # after i+1 inner gradient updates\n",
        "      task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
        "  \n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "      # perform num_inner_updates to get modified weights\n",
        "      # modified weights should be used to evaluate performance\n",
        "      # Note that at each inner update, always use input_tr and label_tr for calculating gradients\n",
        "      # and use input_ts and labels for evaluating performance\n",
        "\n",
        "      # HINTS: You will need to use tf.GradientTape().\n",
        "      # Read through the tf.GradientTape() documentation to see how 'persistent' should be set.\n",
        "      # Here is some documentation that may be useful: \n",
        "      # https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
        "      # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "\n",
        "      # Initial train output, before any update\n",
        "      # They made weights an argument so we can forward with any set of weights\n",
        "      task_output_tr_pre = self.conv_layers(input_tr, weights) \n",
        "\n",
        "      task_loss_tr_pre = self.loss_func(task_output_tr_pre, label_tr)\n",
        "      debug = True\n",
        "      weights_over_iters = []\n",
        "      weights_over_iters.append(weights)\n",
        "      # Update loop based on tr\n",
        "      for update_i in range(num_inner_updates):\n",
        "        # Compute the task-specific loss and gradient\n",
        "        with tf.GradientTape(persistent=True) as inner_tape:\n",
        "          task_output_tr = self.conv_layers(input_tr, weights_over_iters[update_i])\n",
        "          task_loss_tr = self.loss_func(task_output_tr, label_tr)\n",
        "          grad_tr = inner_tape.gradient(task_loss_tr, weights_over_iters[update_i])\n",
        "        # Update copy of weights\n",
        "        #for (k, w), (_, g) in zip(weights_over_iters[update_i].items(), grad_tr.items()):\n",
        "        #print(k)\n",
        "        #print(w.shape)\n",
        "        #print(g)\n",
        "        #print((w - self.inner_update_lr*g).shape)\n",
        "        if self.learn_inner_update_lr:\n",
        "          weights_over_iters.append({k: (w - self.inner_update_lr_dict[k]*g) for (k, w), (_, g) in zip(weights_over_iters[update_i].items(), grad_tr.items())})\n",
        "        else:\n",
        "          weights_over_iters.append({k: (w - self.inner_update_lr*g) for (k, w), (_, g) in zip(weights_over_iters[update_i].items(), grad_tr.items())})\n",
        "        # Evaluate based on ts\n",
        "        task_output_ts = self.conv_layers(input_ts, weights_over_iters[update_i+1])\n",
        "        task_outputs_ts.append(task_output_ts)\n",
        "        task_loss_ts = self.loss_func(task_output_ts, label_ts)\n",
        "        task_losses_ts.append(task_loss_ts)\n",
        "      #############################\n",
        "\n",
        "      # Compute accuracies from output predictions\n",
        "      task_accuracy_tr_pre = accuracy(tf.argmax(input=label_tr, axis=1), tf.argmax(input=tf.nn.softmax(task_output_tr_pre), axis=1))\n",
        "\n",
        "      for j in range(num_inner_updates):\n",
        "        task_accuracies_ts.append(accuracy(tf.argmax(input=label_ts, axis=1), tf.argmax(input=tf.nn.softmax(task_outputs_ts[j]), axis=1)))\n",
        "\n",
        "      task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
        "\n",
        "      return task_output\n",
        "\n",
        "    input_tr, input_ts, label_tr, label_ts = inp\n",
        "    # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
        "    unused = task_inner_loop((input_tr[0], input_ts[0], label_tr[0], label_ts[0]),\n",
        "                          False,\n",
        "                          meta_batch_size,\n",
        "                          num_inner_updates)\n",
        "    out_dtype = [tf.float32, [tf.float32]*num_inner_updates, tf.float32, [tf.float32]*num_inner_updates]\n",
        "    out_dtype.extend([tf.float32, [tf.float32]*num_inner_updates])\n",
        "    task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "    result = tf.map_fn(task_inner_loop_partial,\n",
        "                    elems=(input_tr, input_ts, label_tr, label_ts),\n",
        "                    dtype=out_dtype,\n",
        "                    parallel_iterations=meta_batch_size)\n",
        "    return result\n",
        "   "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1pz_ousUsz"
      },
      "source": [
        "\"\"\"Model training code\"\"\"\n",
        "\"\"\"\n",
        "Usage Instructions:\n",
        "  5-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=25 --k_shot=1 --inner_update_lr=0.4 --num_inner_updates=1 --logdir=logs/omniglot5way/\n",
        "  20-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=16 --k_shot=1 --n_way=20 --inner_update_lr=0.1 --num_inner_updates=5 --logdir=logs/omniglot20way/\n",
        "  To run evaluation, use the '--meta_train=False' flag and the '--meta_test_set=True' flag to use the meta-test set.\n",
        "\"\"\"\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "def outer_train_step(inp, model, optim, meta_batch_size=25, num_inner_updates=1):\n",
        "  with tf.GradientTape(persistent=False) as outer_tape:\n",
        "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts\n",
        "\n",
        "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
        "  result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "  outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts  \n",
        "\n",
        "\n",
        "def meta_train_fn(model, exp_string, data_generator,\n",
        "               n_way=5, meta_train_iterations=15000, meta_batch_size=25,\n",
        "               log=True, logdir='/tmp/data', k_shot=1, num_inner_updates=1, meta_lr=0.001):\n",
        "  SUMMARY_INTERVAL = 10\n",
        "  SAVE_INTERVAL = 100\n",
        "  PRINT_INTERVAL = 10  \n",
        "  TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
        "\n",
        "  pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
        "\n",
        "  val_accuracies = []\n",
        "\n",
        "  for itr in range(meta_train_iterations):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of training data and partition into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    batched_input, batched_label = data_generator.sample_batch('meta_train', batch_size=meta_batch_size, shuffle=True, swap=True)\n",
        "    split_input = np.split(batched_input, 2, axis=1) # [B, K, N, 784]\n",
        "    input_tr, input_ts = split_input[0], split_input[1]\n",
        "    split_label = np.split(batched_label, 2, axis=1) # [B, K, N, N]\n",
        "    label_tr, label_ts = split_label[0], split_label[1]\n",
        "    input_tr = input_tr.reshape([meta_batch_size, -1, 784])\n",
        "    input_ts = input_ts.reshape([meta_batch_size, -1, 784])\n",
        "    label_tr = label_tr.reshape([meta_batch_size, -1, num_classes])\n",
        "    label_ts = label_ts.reshape([meta_batch_size, -1, num_classes])\n",
        "    #############################\n",
        "\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    \n",
        "    result = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    if itr % SUMMARY_INTERVAL == 0:\n",
        "      pre_accuracies.append(result[-2])\n",
        "      post_accuracies.append(result[-1][-1])\n",
        "\n",
        "    if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
        "      print_str = 'Iteration %d: pre-inner-loop train accuracy: %.5f, post-inner-loop test accuracy: %.5f' % (itr, np.mean(pre_accuracies), np.mean(post_accuracies))\n",
        "      print(print_str)\n",
        "      pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "    if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0:\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "      \n",
        "      # sample a batch of validation data and partition it into\n",
        "      # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "      # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "      batched_input, batched_label = data_generator.sample_batch('meta_validation', batch_size=meta_batch_size, shuffle=True, swap=True)\n",
        "      split_input = np.split(batched_input, 2, axis=1) # [B, K, N, 784]\n",
        "      input_tr, input_ts = split_input[0], split_input[1]\n",
        "      split_label = np.split(batched_label, 2, axis=1) # [B, K, N, N]\n",
        "      label_tr, label_ts = split_label[0], split_label[1]\n",
        "      input_tr = input_tr.reshape([meta_batch_size, -1, 784])\n",
        "      input_ts = input_ts.reshape([meta_batch_size, -1, 784])\n",
        "      label_tr = label_tr.reshape([meta_batch_size, -1, num_classes])\n",
        "      label_ts = label_ts.reshape([meta_batch_size, -1, num_classes])\n",
        "      #############################\n",
        "\n",
        "      inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "      result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "      val_accuracies.append(result[-1][-1])\n",
        "      print('Meta-validation pre-inner-loop train accuracy: %.5f, meta-validation post-inner-loop test accuracy: %.5f' % (result[-2], result[-1][-1]))\n",
        "\n",
        "  model_file = logdir + '/' + exp_string +  '/model' + str(itr)\n",
        "  print(\"Saving to \", model_file)\n",
        "  model.save_weights(model_file)\n",
        "\n",
        "  return val_accuracies\n",
        "\n",
        "# calculated for omniglot\n",
        "NUM_META_TEST_POINTS = 600\n",
        "\n",
        "def meta_test_fn(model, data_generator, n_way=5, meta_batch_size=25, k_shot=1,\n",
        "              num_inner_updates=1):\n",
        "  \n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "\n",
        "  meta_test_accuracies = []\n",
        "\n",
        "  for _ in range(NUM_META_TEST_POINTS):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    batched_input, batched_label = data_generator.sample_batch('meta_test', batch_size=meta_batch_size, shuffle=True, swap=True)\n",
        "    split_input = np.split(batched_input, 2, axis=1) # [B, K, N, 784]\n",
        "    input_tr, input_ts = split_input[0], split_input[1]\n",
        "    split_label = np.split(batched_label, 2, axis=1) # [B, K, N, N]\n",
        "    label_tr, label_ts = split_label[0], split_label[1]\n",
        "    input_tr = input_tr.reshape([meta_batch_size, -1, 784])\n",
        "    input_ts = input_ts.reshape([meta_batch_size, -1, 784])\n",
        "    label_tr = label_tr.reshape([meta_batch_size, -1, num_classes])\n",
        "    label_ts = label_ts.reshape([meta_batch_size, -1, num_classes])\n",
        "    #############################\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    meta_test_accuracies.append(result[-1][-1])\n",
        "\n",
        "  meta_test_accuracies = np.array(meta_test_accuracies)\n",
        "  means = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  ci95 = 1.96*stds/np.sqrt(NUM_META_TEST_POINTS)\n",
        "\n",
        "  print('Mean meta-test accuracy/loss, stddev, and confidence intervals')\n",
        "  print((means, stds, ci95))\n",
        "  return meta_test_accuracies\n",
        "\n",
        "def run_maml(n_way=5, k_shot=1, meta_batch_size=25, meta_lr=0.001,\n",
        "             inner_update_lr=0.4, num_filters=32, num_inner_updates=1,\n",
        "             learn_inner_update_lr=False,\n",
        "             resume=False, resume_itr=0, log=True, logdir='/tmp/data',\n",
        "             data_path='./omniglot_resized',meta_train=True,\n",
        "             meta_train_iterations=15000, meta_train_k_shot=-1,\n",
        "             meta_train_inner_update_lr=-1, model_file=None):\n",
        "\n",
        "\n",
        "  # call data_generator and get data with k_shot*2 samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
        "\n",
        "  # set up MAML model\n",
        "  dim_output = data_generator.dim_output\n",
        "  dim_input = data_generator.dim_input\n",
        "  model = MAML(dim_input,\n",
        "              dim_output,\n",
        "              num_inner_updates=num_inner_updates,\n",
        "              inner_update_lr=inner_update_lr,\n",
        "              k_shot=k_shot,\n",
        "              num_filters=num_filters,\n",
        "              learn_inner_update_lr=learn_inner_update_lr)\n",
        "\n",
        "  if meta_train_k_shot == -1:\n",
        "    meta_train_k_shot = k_shot\n",
        "  if meta_train_inner_update_lr == -1:\n",
        "    meta_train_inner_update_lr = inner_update_lr\n",
        "\n",
        "  exp_string = 'cls_'+str(n_way)+'.mbs_'+str(meta_batch_size) + '.k_shot_' + str(meta_train_k_shot) + '.inner_numstep_' + str(num_inner_updates) + '.inner_updatelr_' + str(meta_train_inner_update_lr) + '.learn_inner_update_lr_' + str(learn_inner_update_lr)\n",
        "\n",
        "\n",
        "  if meta_train:\n",
        "    accuracies = meta_train_fn(model, exp_string, data_generator,\n",
        "                  n_way, meta_train_iterations, meta_batch_size, log, logdir,\n",
        "                  k_shot, num_inner_updates, meta_lr)\n",
        "  else:\n",
        "    meta_batch_size = 1\n",
        "\n",
        "    #model_file = tf.train.latest_checkpoint(logdir + '/' + exp_string)\n",
        "    print(\"Restoring model weights from \", model_file)\n",
        "    model_file = tf.train.latest_checkpoint(model_file)\n",
        "    model.load_weights(model_file)\n",
        "\n",
        "    accuracies = meta_test_fn(model, data_generator, n_way, meta_batch_size, k_shot, num_inner_updates)\n",
        "  \n",
        "  return accuracies\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ9VkmS4hlMO"
      },
      "source": [
        "# Prob 1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J35-SCNdXUzE"
      },
      "source": [
        "fixed_lr = 0.04\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo643f6XXfUg"
      },
      "source": [
        "fixed_lr = 0.4\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MRv68PiMLZl"
      },
      "source": [
        "fixed_lr = 4.0\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCrq1Ya-rlyS",
        "outputId": "de6df831-a3e3-4653-bcc1-4f6d48c6a8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.close('all')\n",
        "val_acc_dict = {}\n",
        "colors_dict = dict(zip(['0.04', '0.4', '4.0'], ['tab:blue', 'tab:orange', 'tab:green']))\n",
        "for l in ['0.04', '0.4', '4.0']:\n",
        "  val_acc_dict[l] = np.load('/content/gdrive/My Drive/cs330_hw2/val_acc_lr={:s}.npy'.format(l))\n",
        "  plt.plot(np.arange(len(val_acc_dict[l]))*50, val_acc_dict[l], label='lr={:s}'.format(l), color=colors_dict[l])\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.legend()\n",
        "plt.savefig('/content/gdrive/My Drive/cs330_hw2/prob_1.3.png')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twH-jncXkLBh",
        "outputId": "7c6dc72f-6cd0-4cb9-d4b0-34dfe7d3cd3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for l in ['0.04', '0.4', '4.0']:\n",
        "  test_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=float(l), num_inner_updates=1, meta_train=False, \n",
        "                      model_file='/content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_{:s}.learn_inner_update_lr_False'.format(l))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.04.learn_inner_update_lr_False\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.21300001, 0.136251, 0.0109023508175549)\n",
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.51633334, 0.17701192, 0.014163903651598767)\n",
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_4.0.learn_inner_update_lr_False\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.5686667, 0.17875, 0.014302978331771304)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQx865fquO4S"
      },
      "source": [
        "# Prob 1.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iugRyzSNuQlu",
        "outputId": "52cab563-e544-43bd-e13f-ac72b1a0aef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fixed_lr = 0.04\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, learn_inner_update_lr=True,\n",
        "                   logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/learn_lr_val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Iteration 10: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.20400\n",
            "Iteration 20: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 30: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 40: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.18400\n",
            "Iteration 50: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.18400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.20000\n",
            "Iteration 60: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 70: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 80: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.18400\n",
            "Iteration 90: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.15200\n",
            "Iteration 100: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.25600\n",
            "Iteration 110: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 120: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.17600\n",
            "Iteration 130: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.19200\n",
            "Iteration 140: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 150: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.21600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 160: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 170: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.27200\n",
            "Iteration 180: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 190: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 200: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.28800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.26400\n",
            "Iteration 210: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 220: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 230: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 240: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 250: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.36800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.36000\n",
            "Iteration 260: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 270: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 280: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 290: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 300: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.31200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.38400\n",
            "Iteration 310: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 320: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 330: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 340: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 350: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 360: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 370: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 380: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 390: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 400: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 410: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 420: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 430: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 440: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 450: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 460: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 470: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 480: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 490: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 500: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 510: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 520: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 530: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 540: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 550: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.38400\n",
            "Iteration 560: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 570: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 580: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 590: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 600: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.15200, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 610: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 620: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 630: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 640: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 650: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.37600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.36000\n",
            "Iteration 660: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 670: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 680: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 690: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 700: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.32800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 710: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 720: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 730: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 740: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 750: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 760: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 770: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 780: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 790: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 800: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.42400\n",
            "Iteration 810: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 820: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 830: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 840: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 850: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 860: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 870: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 880: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 890: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 900: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.41600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.44000\n",
            "Iteration 910: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 920: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 930: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 940: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 950: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.15200, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 960: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 970: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 980: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 990: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1000: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1010: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 1020: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1030: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1040: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1050: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.41600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1060: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1070: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1080: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1090: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 1100: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1110: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1120: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1130: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1140: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1150: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1160: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1170: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1180: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1190: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1200: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1210: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1220: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1230: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1240: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1250: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.45600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1260: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1270: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 1280: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1290: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1300: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.25600, meta-validation post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1310: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1320: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1330: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1340: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1350: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.14400, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1360: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1370: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1380: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1390: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1400: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.44000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1410: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1420: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1430: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1440: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1450: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1460: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1470: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1480: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1490: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1500: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1510: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1520: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1530: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1540: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1550: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1560: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 1570: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1580: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1590: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1600: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1610: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1620: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1630: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1640: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1650: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.50400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1660: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1670: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1680: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1690: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1700: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.56000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1710: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1720: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1730: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1740: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1750: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1760: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1770: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1780: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1790: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1800: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.56800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1810: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1820: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1830: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 1840: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1850: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1860: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1870: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1880: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1890: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1900: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.45600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1910: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1920: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1930: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1940: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1950: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1960: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1970: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 1980: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1990: pre-inner-loop train accuracy: 0.10400, post-inner-loop test accuracy: 0.45600\n",
            "Saving to  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.04.learn_inner_update_lr_True/model1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uJaElLuuaud",
        "outputId": "41c5a5ad-6820-41a1-a93f-ef0f5bf4d2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fixed_lr = 0.4\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, learn_inner_update_lr=True,\n",
        "                   logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/learn_lr_val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 20: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.21600\n",
            "Iteration 30: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 40: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 50: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.21600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.20000\n",
            "Iteration 60: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 70: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.22400\n",
            "Iteration 80: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 90: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.21600\n",
            "Iteration 100: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.21600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.14400, meta-validation post-inner-loop test accuracy: 0.20800\n",
            "Iteration 110: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.27200\n",
            "Iteration 120: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.26400\n",
            "Iteration 130: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 140: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 150: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.28000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 160: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 170: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.28800\n",
            "Iteration 180: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.25600\n",
            "Iteration 190: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 200: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.29600\n",
            "Iteration 210: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 220: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 230: pre-inner-loop train accuracy: 0.10400, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 240: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 250: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.30400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.12000, meta-validation post-inner-loop test accuracy: 0.36000\n",
            "Iteration 260: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 270: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 280: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 290: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 300: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.33600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 310: pre-inner-loop train accuracy: 0.30400, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 320: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 330: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 340: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 350: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.27200, meta-validation post-inner-loop test accuracy: 0.33600\n",
            "Iteration 360: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 370: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 380: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 390: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 400: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.40800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.34400\n",
            "Iteration 410: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 420: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 430: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 440: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 450: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 460: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 470: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 480: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 490: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 500: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.33600\n",
            "Iteration 510: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 520: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 530: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 540: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 550: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 560: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 570: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 580: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 590: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 600: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.39200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 610: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 620: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 630: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 640: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 650: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.37600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.44000\n",
            "Iteration 660: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 670: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 680: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 690: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 700: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.40000\n",
            "Iteration 710: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 720: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 730: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 740: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 750: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.38400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.36000\n",
            "Iteration 760: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 770: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 780: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 790: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 800: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 810: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 820: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 830: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 840: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 850: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.41600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.14400, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 860: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 870: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 880: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 890: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 900: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 910: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 920: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 930: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 940: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 950: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.15200, meta-validation post-inner-loop test accuracy: 0.44800\n",
            "Iteration 960: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 970: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 980: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 990: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1000: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1010: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1020: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1030: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1040: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1050: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1060: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1070: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1080: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1090: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1100: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1110: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 1120: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1130: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1140: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1150: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.41600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.13600, meta-validation post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1160: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1170: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1180: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1190: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1200: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1210: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1220: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1230: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1240: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1250: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1260: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1270: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1280: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1290: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1300: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1310: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1320: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1330: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1340: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1350: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1360: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1370: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1380: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1390: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1400: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1410: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1420: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1430: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1440: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1450: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.12000, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 1460: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1470: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1480: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1490: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 1500: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.45600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1510: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1520: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1530: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1540: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1550: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.50400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1560: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1570: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1580: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1590: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 1600: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1610: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1620: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1630: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1640: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 1650: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1660: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1670: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1680: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1690: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1700: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1710: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1720: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1730: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1740: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1750: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1760: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1770: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1780: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1790: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1800: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1810: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1820: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1830: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1840: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1850: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1860: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1870: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1880: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1890: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1900: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1910: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1920: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1930: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1940: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1950: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.14400, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1960: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1970: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1980: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1990: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.46400\n",
            "Saving to  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_True/model1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-lzKy4ubM1",
        "outputId": "b091f1c0-3ab7-4ae9-a78d-a1b43b953042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fixed_lr = 4.0\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=fixed_lr, num_inner_updates=1, meta_train_iterations=2000, meta_train=True, learn_inner_update_lr=True,\n",
        "                   logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/learn_lr_val_acc_lr={:s}.npy'.format(str(fixed_lr)), val_acc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20400\n",
            "Iteration 20: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 30: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 40: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 50: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.19200\n",
            "Iteration 60: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 70: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.25600\n",
            "Iteration 80: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 90: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.26400\n",
            "Iteration 100: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.25600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.26400\n",
            "Iteration 110: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 120: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 130: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.27200\n",
            "Iteration 140: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 150: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.28800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 160: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 170: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.28000\n",
            "Iteration 180: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.31200\n",
            "Iteration 190: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 200: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.35200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.38400\n",
            "Iteration 210: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 220: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 230: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 240: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 250: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.35200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 260: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 270: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 280: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 290: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 300: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.39200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.14400, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 310: pre-inner-loop train accuracy: 0.28000, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 320: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 330: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 340: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 350: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.32800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.13600, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 360: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 370: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 380: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 390: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 400: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.40000\n",
            "Iteration 410: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 420: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 430: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 440: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 450: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.40000\n",
            "Iteration 460: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 470: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 480: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 490: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 500: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.39200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 510: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 520: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 530: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 540: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 550: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.39200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24000, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 560: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 570: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 580: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 590: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 600: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.32800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.36000\n",
            "Iteration 610: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 620: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 630: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 640: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 650: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.38400\n",
            "Iteration 660: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 670: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 680: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 690: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 700: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.38400\n",
            "Iteration 710: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 720: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 730: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 740: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 750: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.36800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.33600\n",
            "Iteration 760: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 770: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 780: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 790: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 800: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 810: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 820: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 830: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 840: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 850: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 860: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 870: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 880: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 890: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 900: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.35200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 910: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 920: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 930: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 940: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 950: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 960: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 970: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 980: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 990: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 1000: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1010: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 1020: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1030: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1040: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1050: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1060: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1070: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1080: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1090: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 1100: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1110: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1120: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1130: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 1140: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1150: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.25600, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1160: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1170: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1180: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1190: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1200: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1210: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1220: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1230: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 1240: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1250: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.50400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1260: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1270: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1280: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1290: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1300: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1310: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1320: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1330: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1340: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1350: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.54400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1360: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1370: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1380: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1390: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1400: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1410: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1420: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1430: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1440: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1450: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1460: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1470: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1480: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1490: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1500: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.56000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.57600\n",
            "Iteration 1510: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1520: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1530: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1540: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1550: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24000, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1560: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1570: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1580: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1590: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1600: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.40800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.56800\n",
            "Iteration 1610: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1620: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1630: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1640: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1650: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1660: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 1670: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.63200\n",
            "Iteration 1680: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1690: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1700: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.57600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1710: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1720: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1730: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1740: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1750: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.58400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1760: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1770: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1780: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1790: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 1800: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1810: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 1820: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1830: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1840: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1850: pre-inner-loop train accuracy: 0.28800, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1860: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1870: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1880: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.61600\n",
            "Iteration 1890: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.59200\n",
            "Iteration 1900: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.49600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.60800\n",
            "Iteration 1910: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1920: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 1930: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1940: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1950: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.56000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.12800, meta-validation post-inner-loop test accuracy: 0.59200\n",
            "Iteration 1960: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 1970: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1980: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1990: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.56800\n",
            "Saving to  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_4.0.learn_inner_update_lr_True/model1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UEjXzTvw1H5",
        "outputId": "76d3fc3a-1a9c-4c7e-fb88-ead471ff6489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.close('all')\n",
        "\n",
        "val_acc_dict = {}\n",
        "learn_lr_val_acc_dict = {}\n",
        "colors_dict = dict(zip(['0.04', '0.4', '4.0'], ['tab:blue', 'tab:orange', 'tab:green']))\n",
        "for l in ['0.04', '0.4', '4.0']:\n",
        "  #val_acc_dict[l] = np.load('/content/gdrive/My Drive/cs330_hw2/val_acc_lr={:s}.npy'.format(l))\n",
        "  learn_lr_val_acc_dict[l] = np.load('/content/gdrive/My Drive/cs330_hw2/learn_lr_val_acc_lr={:s}.npy'.format(l))\n",
        "  #plt.plot(np.arange(len(val_acc_dict[l]))*50, val_acc_dict[l], label='lr={:s}'.format(l), color=colors_dict[l])\n",
        "  plt.plot(np.arange(len(learn_lr_val_acc_dict[l]))*50, learn_lr_val_acc_dict[l], label='learned lr={:s}'.format(l), linewidth=2, linestyle='solid', color=colors_dict[l])\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.legend()\n",
        "plt.savefig('/content/gdrive/My Drive/cs330_hw2/prob_1.4.png')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUVfrHP2dm0jsplFSSQAgBQm8CUpSqoIgC9lV21Z+Fteuua9mirm2V1V1EUVFXsYCCCkgTUHpvoSRASIH03jMz5/fHmZn0ZAIJCeF+nmeeZGbOuffMBO57z1u+r5BSoqGhoaFx5aJr6wVoaGhoaLQtmiHQ0NDQuMLRDIGGhobGFY5mCDQ0NDSucDRDoKGhoXGFY2jrBTQXPz8/GRYW1tbL0NDQ0Lis2Lt3b5aU0r++9y47QxAWFsaePXvaehkaGhoalxVCiLMNvae5hjQ0NDSucDRDoKGhoXGFoxkCDQ0NjSucyy5GUB+VlZWkpKRQVlbW1kvRuAicnZ0JCgrCwcGhrZeioXFF0SEMQUpKCh4eHoSFhSGEaOvlaFwAUkqys7NJSUmhe/fubb0cDY0rig7hGiorK8PX11czApcxQgh8fX21XZ2GRhvQIQwBoBmBDoD2N9TQaBs6jCHQ0NDQuFzJLs3m8U2PsyFpQ5ucXzMELYS7u3tbL8FGWFgYWVlZdr9uD+Xl5cyePZvIyEiGDRtGYmJivePWrFlDVFQUkZGRvPrqq3Xef+SRR9rVd6Wh0R5YemIpa8+u5bFNj/Fd/HeX/PytagiEEJOFECeEEAlCiGcaGHOLECJOCHFUCPFFa67ncsBkMl3S80kpMZvNTY5bvHgxPj4+JCQk8Oijj/L000/XGWMymXjwwQdZvXo1cXFxfPnll8TFxdne37NnD7m5uS26fg2Nyx0pJatOrwLALM08v+15/nfsf5d0Da1mCIQQeuA9YArQG5grhOhda0wP4FngKillDPDH1lrPpeT1119nyJAh9OvXjxdeeMH2+g033MCgQYOIiYlh0aJFttfd3d15/PHHiY2NZfv27bi7u/PnP/+Z2NhYhg8fTnp6OgCZmZncdNNNDBkyhCFDhrB161YAsrOzmThxIjExMcybN4+mus4lJiYSFRXFnXfeSZ8+fUhOTm7yM61YsYK77roLgFmzZrFhw4Y659m1axeRkZGEh4fj6OjInDlzWLFiBaCMxJNPPslrr71mxzeooXHlcDT7KEmFSfi5+PHE4CcAeHXXq3xw6INLtobWTB8dCiRIKU8DCCGWAjOAuGpjfg+8J6XMBZBSZlzsScOe+eliD1Evia9Os2vc2rVriY+PZ9euXUgpmT59Olu2bGHMmDF89NFHdOrUidLSUoYMGcJNN92Er68vxcXFDBs2jDfffBOA4uJihg8fzj/+8Q+eeuopPvjgA5577jnmz5/Po48+yqhRo0hKSmLSpEkcO3aMl156iVGjRvH888/z008/sXjx4ibXGR8fz5IlSxg+fDgAs2fP5sSJE3XGPfbYY9x5552kpqYSHBwMgMFgwMvLi+zsbPz8/Gxjq48BCAoKYufOnQC8++67TJ8+na5du9r1PWpoXCn8dFpdsyaHTeaumLvwcPTgxW0vsmD/Aoori5k/cH6rJ1K0piEIBKrfaqYAw2qN6QkghNgK6IEXpZRrah9ICPEH4A8AISEhrbLYlmLt2rWsXbuWAQMGAFBUVER8fDxjxoxhwYIFfPed8v8lJycTHx+Pr68ver2em266yXYMR0dHrrvuOgAGDRrEunXrAFi/fn0NV0tBQQFFRUVs2bKF5cuXAzBt2jR8fHyaXGdoaKjNCAB89dVXF/nJG+bcuXN88803bNq0qdXOoaFxOWIym1iTqC55U7tPBWBmj5m4GFz4069/YvGRxRRXFvPssGfRidbz5Ld1QZkB6AGMBYKALUKIvlLKvOqDpJSLgEUAgwcPbtTvYe+de2shpeTZZ5/lvvvuq/H6pk2bWL9+Pdu3b8fV1ZWxY8facuadnZ3R6/W2sQ4ODrY7AL1ej9FoBMBsNrNjxw6cnZ0vep1ubm41nje1IwgMDCQ5OZmgoCCMRiP5+fn4+vrWGGsdYyUlJYXAwED2799PQkICkZGRAJSUlBAZGUlCQsJFfw4NjcuZ3em7ySrNItgjmD5+fWyvT+k+BReDC49vepylJ5ZSYizhpZEvYdC1ziW7NYPFqUBwtedBlteqkwKslFJWSinPACdRhuGyZdKkSXz00UcUFRUByl2SkZFBfn4+Pj4+uLq6cvz4cXbs2NHsY0+cOJF///vftucHDhwAYMyYMXzxhYqzr169+oICsl999RUHDhyo87jzzjsBmD59OkuWLAHg22+/Zfz48XW2q0OGDCE+Pp4zZ85QUVHB0qVLmT59OtOmTSMtLY3ExEQSExNxdXXVjICGBtiCxFO7T63z/2ls8Fjeu+Y9XAwurDy1kqe2PEWlqbJV1tGahmA30EMI0V0I4QjMAVbWGvM9ajeAEMIP5So63YpranUmTpzIrbfeyogRI+jbty+zZs2isLCQyZMnYzQaiY6O5plnnqnhlrGXBQsWsGfPHvr160fv3r1ZuHAhAC+88AJbtmwhJiaG5cuXt4r77N577yU7O5vIyEjeeustW2rouXPnmDpVbWkNBgPvvvsukyZNIjo6mltuuYWYmJgWX4uGRkegwlTB+rPrgSq3UG2Gdx3OomsX4eHgwbqz65j/y3zMsuksv+YimsowuaiDCzEVeBvl//9ISvkPIcRfgT1SypVCmcA3gcmACfiHlHJpY8ccPHiwrN2Y5tixY0RHR7fKZ9C4tGh/S40rhQ1JG/jjL38kulM0X1//daNj47LjuH/d/TzQ/wHm9pp7QecTQuyVUg6u771WjRFIKVcBq2q99ny13yXwmOWhoaGhccVQ3S3UFL19e/PDjT/g5eTVKmvRKos1NDQ0LjFFFUVsTtmMQDC5+2S75rSWEQDNEGhoaGhccjYmb6TcVM7AzgPp4talrZejGQINDQ2NS01z3EKXAs0QaGhoaFxCskqz2HF+BwZhYGLoxLZeDqAZAg0NjXZAmbGMJUeXkFOW09ZLaXXWJq7FJE1cFXgV3s7ebb0cQDMELUZ7klZuSxlqUAJzAwYMsMlkaGg0xbL4Zbyx5w3e2vNWWy+l1Vl1pn25hUAzBO2Oy1mG2so777yj1QJoNIv43HgAtqRswWS+tP8HLiUphSkczDyIi8GFscFj23o5NjRD0ApcqTLUoPSFfvrpJ+bNm9f0F6WhYeFswVkAcstzOZR1qI1X03pYBebGBY/D1cG1jVdTRVuLzrU8L7ZSru2L+XYNu5JlqAH++Mc/8tprr1FYWGjX96WhAZBYkGj7fVPyJgYEDGjW/IySDN7Y/QZ39bmLGN/2K2tilZyeFt624pi16XiGoI25kmWof/zxRwICAhg0aJAmOa1hN0UVRWSVVsWuNidv5tFBjzbrGB8f+ZjViavJKcvhw0kftvQSW4STuSdJyEvAy8mLEV1HtPVyatDxDIGdd+6txZUsQ71161ZWrlzJqlWrKCsro6CggNtvv53PP//8oter0XGxuoXCPMPILs3mVP4pkguTCfYIbmKmwmQ2sfrMagB2pe0ioySDANeAVlvvhWKtHZgYOhEHvUMbr6YmWoyghbmSZahfeeUVUlJSSExMZOnSpYwfP14zAhpNcqbgDAA9fHpwVeBVgNoV2MuutF1kl2UDIJGsOVOnt1WbY5Zmm7FqT9lCVjrejqCNmThxIseOHWPECLX1c3d35/PPP2fy5MksXLiQ6OhooqKiLliG+sEHH6Rfv34YjUbGjBnDwoULeeGFF5g7dy4xMTGMHDmy1WSo77jjDiIjI+nUqRNLlyqR2HPnzjFv3jxWrVrVxBE02pTUvfDd/XDd2xB2VVuvpgbWHUGoZygR3hGsSVzDpuRN3N77drvmW9Mxe/r05GTuSVadWcWdMXe26BoTchO4f/39FFZcWOxLIik1ltLZtTMDOw9s0bW1BK0qQ90aaDLUHRvtb9lK/DAf9n4CsbfCjf9t69XU4MnNT7ImcQ1/v+rvjA0ey9VfXY1AsGXOFjwcPRqdW24qZ9xX4yisLOTr677m7jV3U2Is4ccbfyTUM7TF1rhg3wI+OHzxzeSfGPwEd8Xc1QIraj5tJkOtoaHRTkjdq36mtb/UzOo7Ai8nL/oH9Gdv+l62pm5tUpnzt5TfKKwsJLpTNNG+0VwTeg0rT61k1ZlVPBD7QIutcX/GfgBev/p1RgeOvqBj6IQOF4NLi62pJdFiBBoaHZ2KEki3ZJtlHgdjeduupxpSSlvqqK9jEFJKxgaNBWBTyqYm5/90RqVjTu0+lYKySqaETQFUYLalvB2VpkoOZx0GYFiXYbg5uF3Qo70aAdAMgYZGxyftEEhLta7ZCBlxjY+/hGSUZFBqLMXN4MXoV3fyn02nuDr4agB+TfkVo9nY4NyiiiI2JytNf+eKgcS+tJYjpwLo5NyJxIJEjuUca5E1xuXEUW4qJ9wrHB/nplOzL0c0Q6Ch0dFJ3Vfz+fn24x6y7gbKSjoB8N4vCbjruhHqGUpBRQEHMg40OHdD0gYqzBUMDBjEe+uzkBI+257CpNBJQFW65sWyP125hZpb5HY5oRkCDY2OjjU+4BupfrajOIE1PmA1BCUVJt77JYGrg9SuYHNKw2mk1mwhX4aRnFMKQGpeKRGuowBYfWZ1i+gW7ctQhrQ9Zvu0FJoh0NDo6FgNwaC71c92tCOIzzkNgLnCn8ev7YkQ8L+dZ+ntpdKrNyVvqneeTdNfZ2Dzga4A9OqiMoyOnPYh0D2QjNIM20X8QpFS2gLF2o5Ao0k6ugy1lWXLliGEoHYKr5U1a9YQFRVFZGQkr7766kWdS6MFKMmB3DNgcIF+s9Vr6UegnSh8bk1SfvxQz1AeGh/J9NhuVJok6/e74uHoQWJBom3XUJ2fE3/GLM0EOvUnK99A30Av3rg5FoBVh9OYGKqyjazaPhfKmYIz5JXn4e/iT5B70EUdqz2jGYJ2RnuVoQYoLCzknXfeYdiwYfW+bzKZePDBB1m9ejVxcXF8+eWXNbSRNNoAa3ygW39wDwCvYKgsgexTbbsuILe4guRCdZG/b8QwhBA8dm1PDDrBigNp9PdteFdgdQslJUUB8OSkKGK6eRLh70Z2cQVd9Kqgc93ZdVSaKi94jdYYxYCAAXUq6TsSmiFoBTqiDDXAX/7yF55++ukGtY527dpFZGQk4eHhODo6MmfOHFasWGHXsTVaCatbqJvFv92ln/p5/mDbrKca720+gTTkgBRMj1F386G+bsweEoxZQnpaBFA3TpBcmMyhzEMYhBMF2VEMD+/E6B5+CCGY0T8QgD0nnejh04OCigK2ntt6wWvcl97x4wPQAQvK+i7p2yrHPXzXYbvGdVQZ6n379pGcnMy0adN4/fXX6z1mdalqgKCgIHbu3GnX99bRKawo5PuE77mpx02tokOfV5bHilMrmBExo2b7Q6shCLRcyLr2gxM/QdpB6Hdzi6/DXtILyvhs914cwyT+zl1x1Dva3ntkQg+W7Uth7/HOePfSsy99H/nl+Xg5KYl5q5ZQRUFvkI48NbmX7W59emw33lp3kp+PpvHwzCnE58az6vSqC24CcyXEB6ADGoK2piPKUJvNZh577DE++eQTO78Fjdp8cOgDPj76MSWVJdwXe1/TE5qBWZp5YvMT7EzbSU5ZTpWEs5TVDMEg9dO2I2jbgPGCDfGY9BkA9PQNr/FeZ09n7hoZxvubT+Ni6kGR7jhbU7cyNXwqUkqb378sL5ZrojszMKTq33uYnxuxwd4cTM7DrVJ95l+Sf6GksqTZBjirNIukwiRcDa709Ol5MR+33dPhDIG9d+6tRUeUoZ4xYwZHjhxh7NixAKSlpTF9+nRWrlzJ4MFV0iVWqWorKSkpBAYGXvRaOwK703YDEJfd8jGTz+M+Z2ea2nkdzKzm8slLgpIscOkEPmHqta4WQ5B2SBmKNvB7n80u5qvdyeh9VOJCmGdYnTEPXB3BFzuTyMqMwLnzcTalbGJq+FRO5p7kVP4ppMkVc3EPnpwUVWfujNhuHEzOY8sxMwMCBrA/Yz8bkzdyXXjzemhbdwOx/rEYdB3uUlkDLUbQwnREGWovLy+ysrJITEwkMTGR4cOH1zECAEOGDCE+Pp4zZ85QUVHB0qVLmT59erPX0tEoqSyxVbnG58W36LFP5p7k7X1v257HZcdVVeNW3w1YL/iegcowlOZCfkqLrsVe3lp3EqNZ0r1rCVC/IfB2deS+MeEYC5UA4W+pv1FprrQFiSsL+nJD/xCiutQVpbuuX1d0AjafyGRc4IUXl1njAx3dLQSaIWhxJk6cyK233sqIESPo27cvs2bNorCwkMmTJ2M0GomOjuaZZ565YBnqPXv20K9fP3r37s3ChQsBeOGFF9iyZQsxMTEsX768VWSoG+LcuXNMnar01Q0GA++++y6TJk0iOjqaW265hZiY9ts28FJxOOswJovEQ0phCiWVJS1y3HJTOc/8+gyV5kpu7nkzge6BlBpLOZ2vcvPruIVAGYTqu4JLzLHzBaw8eA5HvQ5vrzyABlVCf3dVd3ydumEq96ewopD96ftZmaDcQrJwAI9eU7+7JsDTmRERvlSYzMjifuiFnm3ntpFTltOstdriA50vgSEozoJv74ETq1v/XPXQsfc7lxDrDgBg/vz5zJ8/v86Y1avr/yNXn1v7+axZs5g1axYAfn5+9fryfX19Wbt2bZNrTExMtB3nyJEjTY5viOptKLt161ajF8HUqVNthkFDUb2oSSI5nX+aPn59Lvq4C/YtID43nlDPUJ4Y/ATPb3ue1KJUjmQdUT5ta+podUMAKk5wepOKE/S6tL1z3/j5BFLCrcNC+KVEuRG7e3Wvd6ybk4EHx0Xy6s5o9E6ZvLPvHbLK0jFXenFzn1GE+Dbs858RG8jWhGzWHylmRPcR/Jb6G+sS1zG712y71llSWcLxnOPohZ5+fv2a/0Gby64P4MgyOPodzHgP+t/a+ueshrYj0NBoZaxaNR4Oyo0Rn3vx7qEd53fwadyn6IWeV0a9gquDK339VMbc4azDYDLCeYtOT2Ct1MeuKlXzUu8I9iTmsOF4Bq6Oeu4aFUBOWQ4uBpdG20reOiwEb9kfgENZar2yaADzJ9SNDVRnUp8uOOp1bD+dzagu1wJVtQf2cCjrECZpolenXq2S5VUDKeHwN5bfzfD9A8owXEI0Q6Ch0YoYzUZbAHd6pIqXnMw9eVHHzC/P58+//RmA+2Pvp6+/MgDWXcaRrCNKbrqyBLxDwc2v5gHaIHNISslrP6tkhHtHdafQdB6AEI8QdKLhy5CTQc9jY65FGqsuxteFTyXAs/GECS8XB8b18kdKyM/uiZPeiX0Z+zhXdM6u9V5Soblz+yHnFLgFwDUvqddWPQG//av1z22hwxiCy63TmkZdOuLf8GTuSUqMJQR7BDOiq6p2vZiAsZSSv27/KxklGcT6xzKv7zzbe9GdotEJHfG58ZQmW5IRaruFAHwjwMEVClKUBIUd7Di/gxu+v4FvTn5zQev+/kAqu87k4O3qwO/HhFc1rPcKa3LurEGhuJgssaaKAP40YYJd57QWl605nGerI7BXcuKSCs0d/lb97DMTRv1RtRNFwPoXYcPf1I6hlekQhsDZ2Zns7OwOeSG5UpBSkp2d3SKpse2J6gVJkT5K/fNiXEM/nv6RtWfX4mpw5ZVRr9RIa3R1cCXSOxKTNHE8eYt6sT5DoNNDZ0uMwo4K4+zSbJ7e8jSn8k/x1+1/ZfHhpgsWq/PNnmQe/1qd548TeuDp7MCZfNWw3p52knqd4IGBtyPNDkwJnou3m2OTcwDG9wrA3cnAoZR8hvur7KHP4j6juLK40XnVd3GtviMwm1RsAKCvpcBv8O9g5iIQevj1DVjzDNgpA3OhdIhgcVBQECkpKWRmZrb1UjQuAmdnZ4KCOpawl9UQDAwYSDe3brg5uJFTlkN2aTa+Lr7NOlZqUSov73wZgGeGPkOwZ3CdMX39+nIy9ySHs48xAOo3BKAyh1J2qThBxLgGzyml5MVtL5JTlkOIRwjJhcm8ve9tiiuLeXjAw03q7yzZlsgLK48C8MdrenDXyDCgSn66vtTR+rhn8HjuGdw8JVFnBz2TYrqwbF8KyakhxPrHcjDzIJ/Ffcb9sfc3OO9E7glKjaWEeITg5+LX4LgWIfE3KEpTdR7V/1b9bgEHF5VJtHMhVBTB9QuUEW8FOoQhcHBwoHv3+jMPNDTaCillla+5sxIti/SO5GDmQeLz4ptlCExmE3/69U8UVRYxIWQCN0TeUO+4Pn59WBa/jCMVOeqOsmtVxktRuRF3J8t/eTvjBN/Gf8umlE14OHiweNJidqft5i9b/8IHhz+g1FjKU0OeatAY/GdTAq+tUXGB56ZFM290VQWxtSGNvYbALiqKlcur2npm9O/Gsn0p/HDwPP+8bT73rL2HJUeXMCdqTk0pjmpUF5qrTo3v7wIorTDh4ljrQn7E4hbqe3Pd4r7o62HuUlh6G+z/XLUcnbkI9A4XvIaG6BCuIQ2N9khqUSoZpRl4O3nT3VPdqPTw6QE03z30fcL37MvYh5+LHy+MeKHBi681c+iIkwME9EY6uPLL8Qxm/XcbfV74maW7ktRAO2oJEvMTeX230pV6bvhzdHHrwvUR1/Pm1W/ioHPg82Of8+L2F+s0f5FS8tqa47y25gRCwMs39q1hBMzSTFKBWkeoV9OuIbs49Qu8HAgLRyufu2VNIyN88XN35HRWMc6mnozsNpKiyiIWH2nYvVWf0Nz7m0/R78WfeeiLfVQYm++mWfzbGfq++DNvrq1WvW8shziLKGPfBnSfIifAHd+BkyccXQ5f3dEqbiLNEGhotBJWt1D/gP62C3cP7wszBL+l/gbA//X/v0b75kZ4R+AsDCQ7OHDYJYKpC37jd5/sZs9ZVW3+3qYETGYJAb1BZ4CseHUnXYtKcyV/+u1PlBpLmdp9KlPDq2pDJoRO4N3x7+Ksd2Z5/HJbURuA2Sx56Yc4/rPpFHqd4O3Z/bl1WM0Cx/TidMpMZXRy7oSno2ezvocG2f8ZICH9MCy7F94dDHs/wSArmdZXNa5ZeTCVRwY+AsCXx78kvTi9zmFqN6KRUvLm2hO8svo4Zgk/HjrPHz7bQ1mlfXLxUkreWR/P336Mw2iWLNx8iuQcS0Fhwnooy4fOfcG/kXTY0BFw10pVER4+FnQtf9nWDIGGRithyzwJqLqzvJAdQfWL09AuQxsdazILwk0qmPqvVFXJG+DhxJ+nRhPSyZXknFI2ncgAgxP490JdPI/WOc6iQ4s4nHWYLm5d+PPwP9d5f2TgSBZeuxA3BzfWJK7hsV8eo6SyjKeXHeKTbYk46nX897aBtsyd6rS4W8hUCfHr1e/jnlP+9pzT8MN8eCeWeYbVuFLGyoPn6OXTm2tDr6XcVM7CQwvrHCqlKIXM0kx8nHwI9Qjlrz/G8e+NCeh1qldCJzdHNp3I5K6PdlFUbmx0WVJKXll9nH+tP4lOqA5qlSbJv9ZZ0oettQN9ZzX9GbsNgIf2wPCGYxsXQ6saAiHEZCHECSFEghDimXrev1sIkSmEOGB5zKvvOBoalyP15aJbVSxP5Z/CLO3b4icXJpNdlk0n506EeNQvH2IySz789TRjXvuFHgUFAKR6OfHyjX3Z8tQ4fj8mnNuHq7lLtls6fjXQm+BAxgEWHVqEQPDyqJcbvGsf1HkQiycuxsvJi00pm5j05V18s+8ULg56Ft89mIkxXeqdZzME1VNHpYSz29RFvbkkbYfyfPDrCVc/CQ/thZsWQ0AMFJ4nePff2e78CLeW/I+EjUt4yL0nOgTfnVzG2T2LVNaO5bF/n+oT0t+5M19+/A6Z27/kBsMOlo1O45FuJ/j6d/3o7OnEzjM53PbhTvJKKupdktksee77IyzachqDTvDvuQNZdMdgDDrBdwdSOZl0vkpOos9N9R6jDm7NSy5oDq1mCIQQeuA9YArQG5grhOhdz9CvpJT9LY8PW2s9GhqXkryyPE7ln8JJ70Rv36p/9l5OXgS4BFBqLCWl0D7Rt+o7i4ZiAx9vPcPffzpGRUEmV1UoDZ+IKD23DgvB2UEFKG8ZHIyTQceWk5mcziyqN05QXFnMs78+i1mauTvmboZ0GdLo2mL8Ylg4YTEO0os8GYdH1/V8eu9QRvfwb3BOvRlDh76Gj6fAhpcaPV+9nFD9Ceip2lOiN6i77Ae2wtyvIGgoXhQx37CcqN/mE/7j08woLMSE5L2dr6rMHMtj36HPARh0eju3Jb/Iu47/5m3DAvrvfBS+uo3IH2ay7I4eBPm4cDA5jzmLdpBZWF5jOUaTmce/Ocj/dibhZNDxwZ2DmdavKyG+rswdGoKU8OsPn4CxDEJGgnfd7K9LTWvuCIYCCVLK01LKCmApMKMVz6eh0W44kKkyT/r49anRdAWa7x6ypznKDwdVxew/RxjpU64uTEdz4mrU1ni7OnKDxVXz2Y6z9WYO/XPXP0kpSiHKJ4qHBjzU5NqKy43847tc8hLvAMCl0z56Bzo1OicxPxGoVUOQ+Kv6eXCpksewFynhpOXOOmpKzfeEgKjJcO9aiuasYIvbJH40DedH03CicsIxSFjt7saJ6EkQcyPE3Mh+L5UuWlzSgzVyBNlh02zv4R0K6UcI+n4my24LIdzfjeNphcx+fzvn8koBKDeaePCLfXy3PxU3Rz2f/G4o43pVSWg8PD4SZwcdEWmWNdvjFroEtKYhCASq90BMsbxWm5uEEIeEEN8KIdreNGpotAD1xQesWA3ByTz7pCaakkPOKCjjYEo+TgYd49yTCTKa8BaO5JTlcK64pqTCHSPUxffbPSkUd4q2HCAOTJVsOLuB7xK+w1HnyKujX61jwGqTX1LJ7Yt3sv10Nn4OEUT7xFJqKubHUz82Oq9e15B1V1KcCWc215nTIFnxKh7g4gNBDcRPhMC911jGPPk1ofd9zU9RL/Nc6WOU5F4FwH3ljhwfvYDUSa9zWpYhzQbeNz9Jt98vxffuL+DmT9Rj3gYV2M1OoPM3N/Lt7C5Ed/XkdFYxNy/czvG0Aib3ADoAACAASURBVOYt2cPPR9PxdDbw+bxhjIio6c4J8HTmwaFejNIdxogeetefBnypadIQCCEeFkI03fLqwvgBCJNS9gPWAUsaWMMfhBB7hBB7tKIxjcuBxrRqmrMjyCnLIbEgEWe9M718e9U7ZuNx1enrqkg/HNL2I4A+lljC4ayajZr6BHoxONSHwnIj3x8rBJ/uYKogM2UHL25/EYDHBj9mq4JuiKyicuZ+sIP9SXkEervwzf0juKfv7YDKyGmoyr/cVM65onPohZ5gd8t9n6kSMo5VDbJKLtiDdTfQY5JyCTVB3yAv/nv7INY9ejUTA29Fmh3JlgeZ9v5nTP/wfwDoKkP5+g+j6RdUq87A3R/u/gECB0N+Ep2WzuDrG30YEOJNal4pU975lV/js/B1c2TpH0YwIKT+y+Y8n0MYhJnNpn78ds4+NYTiciP3f7aXo+fy7RrfXOzZEXQGdgshvrYEf+1taZQKVL/DD7K8ZkNKmS2ltDrYPgTqLYOUUi6SUg6WUg7292/Y96ih0R4oM5ZxJPsIAkFsQGyd95uTQmp1C/Xz74eDrv5CovXHlCGY0Mvf1oOgr8W3fySzrtz4nZbq3k+3nUVa4gQfHllMXnkeI7qOYG6vuY2u6Xx+KbPf307c+QLC/dz45v4RhPq6MSFkAv4u/pzKP2XryFabpIIkJJJA90AcrIVRmcfBVAHOqicxx36AytJG12DDGh+ImmzfeAuRAe78+5Yx3NZLubScO/9MEervcXPM6Hob3gBq53Hn9xA2GorS8Pjyer6Y5szICF+khK5eznx9/wh6d2s4LdblhGpXu8I0ktd/Pt6kNE5+aSV3LN7JmqNpPP71QczmlpfSadIQSCmfA3oAi4G7gXghxMtCiIgmpu4GegghugshHIE5wMrqA4QQXas9nQ4cQ0PjMudo9lGMZiM9fHrUm3ET7h2OXuhJKkyizFjW6LGaUsEsqzTxW4LaJU/sVg4l2eDqS5+gUUDdHQHA5Jgu+Hs4cSK9kGSnSIqFYEW2yhx6fPDjjaqBJmWXcPPC7ZzKLKZXFw++um8E3bxdAHDQO3BzT1UYtfTE0nrn1ys2Z81aCh+n0iQrCuHkzw2uwUZJDiTvAJ0DRNgnRFebhwbdi5eTFzqXM3j6KyM6LmxY45OcPOC2b9QupDQHly9u4OMJJv41O5YVD15FhL97w3Nzz0LyDqTBhf0uIziYks/PR9MaHJ5dVM7cRTvYZ9l5Lbx9EDpdy7cXtStGIJXJSrM8jIAP8K0Q4rVG5hiBh4CfURf4r6WUR4UQfxVCWPsXPiKEOCqEOAg8gjI0GhqXNU0Fd530ToR4hmCWZpv4WlPHqi/WALDtVBZllWb6BHriX2C5+w8cRB9/JSp3LOdYVetKC44GHXOHKtfRynQ/fnB3o1gaGRgwkKhODRc2JWQUcvP720jJLSU22JulfxiOv0fNwPCsnrMwCAMbkzaSVlz3AmeND9QIFFuD1V37VVXYHrZD5TR+ndLvD7sKnC+sMM3D0YN7+9wLQJm5WO3i/Ovu4urg4AKzP1c+/vICnL6cxY2e8U3KY1sF5kSvqfz+GlUF/sbak6rIrxZp+WXcUmvnFebnVmdcS2BPjGC+EGIv8BqwFegrpXwA5cZpNAFWSrlKStlTShkhpfyH5bXnpZQrLb8/K6WMkVLGSinHSSmPX/Qn0tBoY2wSBdaLt7GC9E0fsPvNm0j+5SMwVVa5hxqRpC41lhKXHYdO6OjnX3+XrCq3UOcaHck6OXeyta48lXeqzrzbhoVg0AmWJHrxpadyg8yNmtPgWo6k5nPL+ztILyhneHgn/jdvGN6udYPJ/q7+XBt6LSZp4usTX9d535oxVCN11Boo7hoLMTMBAfFroTSvwfUAVfGBnlMaH9cEc3vNJcBFZfb09OmJh2MDbqHaGBxh1kfQ/3bV++GLW2D7e0oTqCEOV2kLzRkSQpCPCwkZRSzfVzOVOCm7hJvf31bvzqs1sGdH0AmYKaWcJKX8RkpZCSClNAPXtdrKNDQuQ8zSbBMtG+gTDTsWwoL+dN70BEMK1xO8+VHK34qlR7Eq+mosTnAk6whGaSTKJwp3x7ruBiklGy2G4JroznV6FNt0h7Lqxgk6ezozqU8XcpyzOO3ogL/RyATP+r29e8/mMPeDHeQUVzA2yp9Pfje0UfG1Ob2UQVkWv4wKU82CqzpVxWYzpFncV11iwbMrdB+tYgbHfmjwHBgrIGGD+r2Z8YHaOBuceXjgwwCMCRrTvMk6PUz/Nwy9T6355z/B231gy+t1DVn6Ucg4Cs7eEDEBR4PO1nf57fXxlBuVbIV155Wc0/DOq6WxxxCsBmzdK4QQnkKIYQBSSs2nr6FRjYS8BAorC+mqd6PLBxNgzdNQkMopgnirchanzF1xKk6lx1F1kYs/vQ7KCuo9VlNpo0fPFZBWUEZnTyf6dHWFc5bWlN3UTsTasay+OAHAXSPCcPDZDsDNhUU4pMfVGbM1IYvbP9xFYZmRqX27sOiOwbYCtYYYEDCAKJ8ocspy+Dmxpq+/Towg94ySWPboqrJywD73UNI2KC8A/2glKXGR3BB5AytuWMEDsQ80f7JOB1P+CXO+UEa4JBs2/h3+1QfWvQBFyljbdgMxN6jdBHDDgEB6dnYnNa+UL3Ym1dh5De3e8M6rpbHHEPwXqN5dvcjymoaGRnWKMjiw+W8ADMjPUBeEwEHEj3+fa8peZYX37XzY70seqJgP5Z0BiC88q+4gN/4dirNqHM4Wa+hcvyFYf0yJpo3v1RmReRyMpeqiaJEiaGxHABDkV4aDRxxCwqzCojpSE+vj0vndJ7sprTRx08AgFswZgKOh6UuGEMKWebT0eFXQOK8sj7zyPFwNrvi7WC761nN2qeb6ir4e9I5wZgsUNhBIvcBsocYI9wqvymRqLkJAr2mq1uDOFdD9ahX03vq2Mgg/PV5NW6hKaVSvEzw+UcVlFmyIt+28ru7pz5Imdl4tiT2GQMhq+U0Wl1CH6GOgodFinFwLb/dlX+pWAAa6h6gLwrwNfFsUi0THNdGdefmm/nQdMYd7i/+Gzqwnw2Agv6JQuRLe7gtn1R26yWyyVScP8K/fEGywuYUCIGWPerFac5NenXqhF3oS8hIoqazrt/42/lsQEt/CIPxN5hpSEysPnuP+z/dSYTRz54hQXp/VD4Pe/vrTqeFT8XD04FDWIZshqh4otmWhp1ULFFtx8YEeEwEJR5bXPXj1auKLjA+0OEIohdC7VsK8jdDrOjCVw+4PIT8ZPLopWYlqTOzdmf7B3uSWVFJYZmRKny4sunNQ3d4FrYg9f9nTQohHhBAOlsd84HRrL0xD47LBWKGajRvL2O+ucuH7X/++uiAIYbtznxAdgBCCv1wXzSPjo6gsV9nTn8e+AN3HqICjpWF5fF48xZXFBLoH0tmtc51TpheUcTg1H2cHHVdF+lUJmIWMsI2p0boyp2YeRrmpnG9PKleFKBoNQGWqujtfuiuJ+Uv3YzRL7r86gpemxzQ7ZdHF4MLMyJmAKjCDBlRHrRlDXWoFw61CbEfqKS7LPAG5ieDqC0GDm7WuS0rQIJjzP/i/HdBvDhicYeRDdWSk1b+J3vi4OjB3aDD/njsAJ8OlMwJgnyG4HxiJKgZLAYYBf2jNRWloXFbsWwJ5Z0nz78E5jHg4eBDprSpzE7OKOZVZjIezgSFhnQD1H/+xiVH0sWjQv308kyXdnlcdxU5tgOLsuplHtbDuBkZF+uNckafmibqSBQ3FCdYmriW3PJdenXoxNmY6BdIVh9JMvtiwi2eWH0ZKeHJSFM9M6dVkO8qGmB01G4FgzZk15Jbl1o0PSFn/jgCUgJyjuwqAZ9fKeqpeTdxKrRtblIBomPk+PJcOIx6sd8igUB/2Pnctr8xs3s6rpbCnoCxDSjlHShkgpewspbxVSplxKRanodHuqShWbh1gfz+lqRgbEIvecoGy7gbGRgXgUOs/+PXRyo2jc0rjhQ0ZnPEaCmYjxH3fZHxgg+W410QHQNz3al742KqAqwVrnOBoVs2eA9a79Lm95nLnyO7ESZXXv3bDWgCev643D45rXGaiKYI9gxkdNJoKcwXL45fXFZsrTFPaQs5eStCtOo6uyq0CVc3drVh3Py0YH2gPtEahmN3nbmqAEMJZCPGgEOI/QoiPrI9LsTgNjXbPzvehKB26DWCfg/rvVP0uvoYfvxZWzaGIwEJ0AhZk9AdAHv6m0R1BaYWJ3xJUYHl8r4CqC2U97Q7r2xEcyTrC4azDeDp6MqX7FML83Mj3UgJ0N+q38trMPtwzqmV6gFuDxl+d+IrT+cqjbNsRpFVzC9W367B+nkNfq90DqIB68i4VTI4Y3yJr1LDPNfQZ0AWYBGxGaQYVtuaiNDQuC0pzVVYIwITn61QU55dWsjsxB71OMLZnPYbAUlSWU5nEO3P6s9Y8mDLpwLlzu8gozcDT0ZPuXnUvyFsTsig3mokN8iJAZsHZrcr/3GtanbER3hE4651JKUoht0y1q7TuBmb2mImLQRUphY//HRU4MEO/jVtS/tE8KehGGNltJCEeIZwvPm8zBKEelrv/huIDVsKvBlc/yI6vMhrxawEJYaOU1INGi2CPIYiUUv4FKJZSLgGmoeIEGhpXNtv+rXrOho2mMGgwJ3NPYtAZbHfhm09mYjRLBof64OVaNy3R18WXTs6dKK4sZmAEzBzei/XmQexzUsVDAwIG1Kv7s+G4NfjcuWo30HNyvTILBp3B1hjnSNYRcspyWHNmDQLBLVG32Mb1GDAGx7uWg4MbHPoKvrlLNVe/SHRCx+yo2bbn/i7+VcVxaZbU0drxASt6B9UHAKpSL0+002yhyxx7DIG1d1yeEKIP4AXUvb3R0LicqSyDxN/svxMuTIcdlnKaCS9wMOsQEkmMbwzOBqU3U+XHr5v1Y6W6JPWfpkazw208B5yrDEFtzGZpczdNiA6oNze9NlbDdCTrCMvjl1NhrmB00GiCPWq1/+g+RqW8OnvB8R/hyzmNyyVUp7K0wVaTMyJn2HYeNcXmmtgRQLXismXqHKc2qucdLD7Q1thjCBZZ+hE8h1IPjQP+2aqr0tC41Pz6JnwyDb66TRmFJse/odI9o6ZB8BD2pKk8fuvF22gys+mEUgWdUE98wEp1SWoXRz1z5v6OPc7qoumaXbdc58i5fDIKy+nm5Uxvwzklz+DkBT2ubfAc1oDxgcwDNv2fBqWmg4fA3T8pl8ypjfD5TQ1WPgNqR/TrW6oG4uMpsOmVOkO8nLyYFq7cVt09La6u0jzIO6tcWn49Gz5+8FDwDoHCcyooX1GkehF719+7WePCaNQQCCF0QIGUMldKuUVKGW7JHnr/Eq1PQ+PSEPe9+nlyDXxxM5QXNTw2NxH2fAwIGP8cAJtTVFetEd1UHv+es7nkl1YS7udGeCOyxNZm9lbNoeAuTpx2NOBolhRu+YXc4ppaPVaRufHRAQirW6j3dDA0rEVj3RFsO7eN88XnCfEIYWS3kQ2Op0tfuGeNKn5K2gafTleSz9UpyoQNf4V/9VV9hostDaMOfAFmU51DPjzgYW6Pvp27Y+5WL1j1hQJ6N95QRgjoY2nn+JslHqPtBlqcRg2BpYr4qUu0Fg2NtiH7FGSdBCdPcO+spA0+u7Fh9ctNr4K5EvrdAp17k1qUSkJeAm4ObgzprBrCbKhWRNYYNteQRYX0YKbym/epKGeScSvPLjtUo3GJ7bi97HMLAQS6B+LjVNUta06vOY32HADArwfcs1pJVpzbDx9PVemeecmw6im1A/j1TShXMRJuX67GFp5XwetadHLuxNNDnybY0+KOaqh+oD6sn09aDIwWH2hx7HENrRdCPCGECBZCdLI+Wn1lGhrNILcsly+Pf1lH7dIurAHIHhPhd6vBKxhSdsGS6+ro/5BxTDVY1xlg7LMAbEreBKgMGatWTZUfv+H4AKisHoEgMT+RSlOlLW20v8lAqC6DtGNb+Xavkig+n1/K0XMFuDrqGemcqHYm7l1UBk0jCCFsuwIXgwszImfY862oC/vv1oB/L8g8BgtHw4L+sOt9pWvUcwrcuw7u/hEiJ1TdudvTS8Ce+ICVzr2VOwjAzb+GjIZGy2CPIZgNPAhsAfZaHntac1EaGs3lw8Mf8vLOl/ks7rPmTz5pFTCbAr4Ryi3iG6ncFx9PgYJqDeA3/h2QMOhu6KT83ZuTlVtobPBYAE5nFnE6qxgvFwcGhzbe7tvF4EKwRzBGaeR0/mlbCuqgQOW6maHfyosrj5KUXWJzC42K9MPpmEWDp89NdlXXDuys6hGuD7++3q5pDeLZFe5epXoFFGeoRjB9ZsH9W+HWpcqHb8V65x63oumMo+o9COyhnyXDKWpqHYkGjYunSfE4KWXLVJZoaLQiCXkJAGw/t517+95r/8TSXJXtIvTqrhbAK0jtDD69QenHfzRZZdOUZKtsGoMLjHkSgKKKInan70YndIwOVJo91t3AuCh/u+QCevj0IKkwibjsOFvhV2zs3bD/W2Y67eLvxbfz6NcHcLWIkF3byw82WwxB31l2fczbo2/Hz8WPiaET7f1mqnDzhbt+hKPLlRvIt4EutQG9oHNfSD8MCevrrWsAVPZP5gkQOhUjsIcRDyq3nRYfaBWaNARCiDvre11K+WnLL0dD48JILkwGlHRzmbHMlsLZJAkblO85bLRSvbTiHqBcHv+bpfRuPp6iNPMBht0HHl0AFYA1mlWbRx9nNb9KZK5xt5CVHj492JC0ge8TvqfSXEmkdyReoaOgUwReOaeY4hbPj2eVERACrnU9qe7OO0WoHr924Gxw5obIG5oe2OABPNUuqCn6zlKG4PA3DRuCjDj1nfv3UlIS9qB3gP4NZDppXDT27LGGVHuMBl5ENZrX0GgXVJorOVek3DcV5gqbe8UubAVK9dxpunZSO4HQUSoIem6fStUc9UfbEGu20NXBVwOQV1LBnrO5GHSCq6P86x6zHqwppPsyqslKCGFztTwXWqUTFBvkjXfCCvWk7831SzO0JVbV0BOrobwBAYLmxAc0Lgn2iM49XO3xe2Ag0HA+nIbGJeZ80XlMsiplccf5HfZNNFVCwjr1e1QDmShOHnDbNxBpydO/+knbzsFkNvFryq8AjA0aC8CmE5mYzJKh3Tvh6WxfkxNr5pAVm9Ccxe3TJWUt941Uu5Eb+/jCsZU13m9XeAcrKWxjGRxfVf+Y5mQMaVwSLiTqUgxocQONdkNSYRKArXp15/mddk7coQqi/Ho27PcG5b649Wt4aC+MfNj28qGsQ+SW5xLsEWzTBGquWwggxCMEJ31VHYBNaM6vB3TtDxWFPBORxKpHRnO73wnVorFrrHq/PdK3iewhbUfQ7rBHffQHIcRKy+NH4ATwXesvTUPDPpIKlCEYHzIeB50Dcdlx5JfnNz3Rmi1Un1uoNjod+NWUZbamjV4ddDVCCCpNZjafVIVV9amNNoRepyfcKxyAzq6d6erWtepNy0VVHPmW3t080VsbtTRRO9Cm9L5Rpdee2lg3/dZsUk3cQRWuabQL7NkRvAG8aXm8AoyRUj7TqqvS6Hgc/wneGw5p9ffPvRisgeIonyhi/WORSHan7W56ok3XfgobjqUz/s1N7D2b0/icatROG919JofCMiORAe6E+ro16zNY3UMDAgbUbAQTMxMQqhVmXjKc/Fk9j5nZrONfUtx8lUS0NMHRWveMWfGqBsErRMVgNNoF9hiCJGCnlHKzlHIrkC2ECGvVVWl0PDa9qoqSNre8TJXVNRTiEcKwrkoYt8k4QVY85JwCFx/Kuw7i+RVHOZ1ZzF++P4rZLBufizI+p/JP4eHgYcvRX19dDK6ZTAqbhKPOkesjrq/5hlegKhgzlcP3D6ifYaPU6+0Zm1hcrVaTWnygXWKPIfgGMFd7brK8pqFhH5knqy4Ax3+C/NQWPbzVNRTsGczwrsMBO+IE1aqJv9h9jtS8UgDizhew6sj5Js9p3Q1cFXgVDjoHpJQ2eejG1EYbYkzQGPbesZcxQWPqvmn1uSf+WvN5eyZqqqq3SN4BuWerXj9vlZ62s5BM45JgjyEwSCltdfuW3x1bb0kaHQ5bA3Kh3AV7P26xQ5vMJlKKlARDkHsQffz64ObgRmJBImnFaQ1PtMQHysIn8u5GVYw2KUZdwN9aexKjydzgVIBNKZuAqrTRU5lFnM0uwcfVgYEhjVcTN5vo6aCzZCDpHNTz9o6TO/Saqn6v3mrSagi0QHG7wh5DkCmEsP3LE0LMALIaGa+hUYWUVdkjYy2hpb2ftEjTE4C0kjSMZiMBLgG4Orhi0Blswm8NuodKclTGkM7AkvQIsosrGBDizbu3DiTM15XTWcU2fZ/6KKwoZG/aXvRCb6smXr5P7XLGRQWgb+nes66dqmSme1x7+fjWa7uHGmtWr9Gm2GMI7gf+JIRIEkIkAU8D97XusjQ6DOf2Qc5pJQ8w5kklQVCcqfRoWoDqbiEr1jhBg+6hhPUgTVQGjeTdbcqv/+SkKBz0Oh6bGAXAOxviKausK6cMsDV1K0ZppH9Af7ycvDiQnMf7W1QbxtlDguudc9GMeUKJrY15onWO3xpETABnbyXTkR4HeUkqXdfVr6pKW6NdYE9B2Skp5XCgN9BbSjlSSpnQ+kvT6BBY7wat4mhDf6+e71rUIoe3ZgyFeFQ1KrHGCXac31FDwtmGJT6wiUEUlhsZ3cOPkRF+AFzXtyvRXT05n1/G5zvO1p1LlVtobNBYSiqMPPrVAUxmyb2jujMs3LdFPlcdAgfB7zdeXsqbBkfobVE6PfJtzd1Ae6uIvsKxp47gZSGEt5SySEpZJITwEUL8/VIsTuMyx2yq8g9bA5x9b1atEFN2Q+q+iz6FdUfQzS2IvWdzMJklEd4R+Ln4kVWaZWuYbsNYoXYEwKunwwB4wrILANDpBE9OUs1i3vslgcKymq0XjWajrZr46uCr+ftPxziTVUxUZw+enBSFRi1s7qFvtPhAO8Ye19AUKaWtQ4eUMheY2npL0ugwJP4KRenQKRy6WaplHV1hwB3q990fXvQprKmjy3aWcdN/t/N//9tLhcnccBpp0jYoLyDdKYxTRn8mx3QhNti7xpBxUQEMDvUht6SSxb+dqfHegYwDFFQUEOoZyqlUV77YmYSjXsfbc/rj7NC0HPQVR+hI1eksL0l1LwMtPtAOsccQ6IUQtvp3IYQL0HBfPA0NK9YgcZ9ZNV0Bg+8BhHIbFWdf1CnO5Cv3zalzSm3056PpzFuyh4H+DQSMT6hsoe9L+qET8MSkuv1yhRA8NbkXAB/+eoacau0irSJzQwNG8fQy5ep4anIU0V2bofF/JaHTQx9L8VuBJW24i5Y62t6wxxD8D9gghLhXCHEvsA5Y0rrL0rjsMZZD3A/q99p5774RKvvFVA77L1zN/FxeCYn5KkYQ5BHE4rsG4+vmyK/xWXy5Wd2r7Enbg9FsVBOkhJMqPrDWOICZA4OIDPCo99hDu3dibJQ/ReVG/vNLVUjMKitxKD6I7OIKRkb4cs9VmvRWo1SXw3B0VztEjXaFPcHifwL/AKItj79JKV9r7YVdceSnwJ6P6m38fVkSv071s+3SD/zr8Z0P/YP6uXsxmE1kF5XzydYzJGWX2HX45JwSbv5wDYhKdGYPlt3Znwn537F8The6eDqz/4zAYA6gqLKIo9kWbZvME5CbSLb04Kguij9e07homzV28OmOs5zLK+VswVkSCxJx1rmz57gnns4G3rg5Fl1Lp4t2NLrGgq/lu+7cR+sw1g6x6y8ipVwtpXzC8vi5tRd1RfLjY/Djo3BkeVuvpGVoqrF6xATw6Q75yXByDS+vOs6LP8Qx9o1fmL90P8fTCho8dEJGIbMWbiOtROX69/YPx3/HK7DmaUK/GMOGsE8Z551OSb66U9+YaGmmbtkN/GIewJxhYQT5NN4UpU+gF9f160qF0cyCDfG23UBJfiSg5x839qWbt4t938eVjBDQb7b6PWhw265Fo17syRoaLoTYLYQoEkJUCCFMQoiG/5dqNJ/yIjj9i/o92U4J5fZMWYGlcldUNSqpjU5nSyU17/rAJs8ghGDFgXNMfvtX5i3Zw76k3BrTjqTmc8v7O0gvKCe8WxkA4V4hVZIRCNxOfs/HZY/yiOkYAJ8eWEdKbgmFh34E4FcxmAfH1VQSbYjHru2JXif4Zm8Kq09vBKC8IJobBwRyfWw3e78Rjavmww0LbS0+NdoX9uwI3gXmAvGACzAPeK81F3XFcfoXMFkCkql723YtLcHxn1RjktCrGhdH638rOLiiO/0LvqWJdPdzY/OTY7l7ZBhOBh3rj6Uz8z/bmLtoB7/GZ7L3bC5zP9hBTnEFY3r6M7Gf6rQaLPWqSM07BP54CIY9AAYXbi+OQ0gJDgm8959XcM3YR7k0ED78evw97Mt3CPd35+ZBQZgo5mj2AaTUEaDvx0szYlrim7pyMDiqVpMu3k2P1bjk2OsaSgD0UkqTlPJjQOsg3ZKcWMMeZyf+4teJ/IwjLSa/0GbY3EJNiKO5+NhcR7fr1zOhVwBBPq68OD2Grc+M5//GRuDhZGD76WzuWLyLWQu3UVhmZHJMFz64cxDnLa6hkHyLSFzPKarx/JRX4dEjeI16nOhKE0YB1xsWocfMXhHD3eOap4M//5oeOHsdB2HGVBLGv24eYXf3MQ2NywF7DEGJEMIROCCEeE0I8aid8zTswWymPP5nnvX35XsPd5Z4uLSKZv8loygDTm9SjUmsVaWNYXEPzdJv4drIKg1/P3cnnprci63PjufJSVH4ujkiJcwcEMi7tw7AyaCvkpdIPawmRVW7P3Hzg/HPMazfXQBsdvECwNh7Fl4uzbuInynej3MXJYkxosv41qse1tBoI+y5oN9hGfcQqk1lMNCA47cmQojJQogTQogEIUSDzWyEEDcJIaQQ4sqLJKXu5Wt9GWkG5eZY5uFORfKuQ0knjAAAIABJREFUNl7URXD0e6UwGnmNXeJoiYZwdpp74SFKGZxfNw/B09mBB8dF8tvT4/nhoVG8eUssBr0OKWVVH4KMk+DooZrM12J4sJJ13h/Wj6TZ6xlz88N1xjTGxqSNPLThIUyUc3XXqSya8WCz5mtoXA7Ykz56VkpZJqUskFK+JKV8zB6tISGEHhVLmILSKZorhOhdzzgPYD7QAaKkzaf4+Eo+9FbFSJ46J3L0en5OWtfGq7oImsoWqsX6Y+l8apwIgH73hyrXvx5cHPX0DfKyde/KLsum1FiKp84JL7MZIscrP3QtBgQMwEHnwPHck3h279EsjZtVp1fx2KbHqDRXcmuvW1lw7SsY9Aa752toXC605r/qoUCClPI0gBBiKTADiKs17m/AP4ErMp3gs8RV5DjqifUI48agcbx47GOWFidwfdNTa/Bd/He8s+8dTLLhOgQ3BzfeGvsWvX3r2OOWITcRUnaBgytETbFryvpj6ewxD6bM2R/nrBNwZjOEj21ynk1szto2oGf953MxuNA/oD+703azK20XE8Mm2rWub09+y1+3/xWJ5Pd9f8/DAx6u2UJSQ6MD0Zq+/kAgudrzFMtrNoQQA4FgKeVPjR1ICPEHIcQeIcSezMzMll9pG5GXfpglBpUCOX/Yn5ja7148TWYO6SVHUptotViN/PJ8Xt/zOtll2eSV5zX4SC1K5ektT1NSaV/RVrOxCsz1mgaOTffszS+pZHdiLlLnAIPuUS/u+sCuU9niA8V5IHTQo+ELvN1dyyx8evRTXtr+EhLJ/IHzeWTgI5oR0OjQtFnQVwihA94CHm9qrJRykZRysJRysL+/f+sv7hKxeNfrFOl0jNR5MCRwBC7OXsxEXUC/PGTfBRFgydElFFYUMqTLELbM3lLvY+PNG4n0jiSxIJG39r7VOh/IKjltp1to08kMTGbJkDAfnIffq7pvnVilmrQ3gS0+UFEBQUNVw/QGsLePsZSS/x78L6/veR2AZ4c+y7y+8+z6LBoalzNNuoaEED1RbpvQ6uOllOObmJqKCixbCbK8ZsUD6ANsstxtdQFWCiGmSyn32LX6y5j04nS+zDkAwCMRVWmWt/gPYUn2ZtZk7uGJslx8nFXbQyklpZUmXB1r/smySrP4/NjnAMwfON82vj5eHf0qc3+ay1cnvmJM0Jj6++M2RmF6Vb1DbXLPQEYcuHSCiKb+aSg2WJq9XxPdGTw6Q+/palex92OY8Hyjc5MLLK4ho7FmtlA9xPjG4O7gTlJhEoczD+Pn4lfvuC+Of8EnRz9BJ3S8NPIlboi8wa7PoaFxuWNPjOAbYCHwAapxvb3sBnoIIbqjDMAc4Fbrm1LKfMD2P1IIsQl44kowAgAL9/2bciTX/n97dx4fVX0ufvzzTCYLWSGBkBCysu87goALoIICVrRF64LWq7XWWvV3b2trrdqWe916W21Fi61VW6u9KlasgCKyKIKsYZUlJAQSlrBm3ybz/f1xziQTSMIkZEhgnvfrNa9Mzpk58+QkmWfOd3m+pWUMGDK7dntyygQm5H3CyvAOzN8zn9n97+LjrYeYu2wv2cdKeOW2EUzyWhx93pZ5lLvKuTL5SoZ0abqqY5/YPvx4+I95fv3zPL7qcebPmE9cBx+HQn75O/jsybM/bsC3IOjswzOra9ws32UlgtqfZ/S9diJ4HS77CQSHNfr8/cVW1dGU6mprofQmOB1ORiaMZPmB5Xx34Xebfqw4efqyp7km7Zqz/gxKXSx8SQQuY8zLzT2wMcYlIg8AnwBBwGvGmO0i8itgvTFmQXOPebHILcrlg+yPcBjDA6Gp9Zs1kkZwS1ExK8M78NqWt3h9URoHTlTU7v6v97aw+KEJxEeFkVecx7u730UQfjTMt2GRt/e/nZV5K1l7eC1PfvUkL0580bf2703WVQeRCdYcgYaERsEl9/kUx/p9JymqcJHRJYL0znZ/QvIlkDAIDm+FHf+CITc3+FxjDPsLrXUCkiO6QeczS0mf7tZ+t5JTmENlTeOT9SKDI3lkxCNM6D7Bp59BqYuFL4ngIxG5H/gAqP0vMsacONsTjTELgYWnbWvwmt8Yc4UPsVwUXsp8iRrc3FBSSsbQ+uODSiJTGVHtJKW6mv0cpbxqA2lxo7jv8h58tOUgq7KO89P3tvDanaN4efPLuNwupmVMo1enpitpejjEwZzxc5i5YCbL85bz3p73+Hbvs7TpH8uC41nW+rMPb4dWGEK59BurttBkr6sbRKyrggU/spaybCQRFFYWUuwqJ8LtJrbXdT4NCR2TOIZ/3/Dvc45bqYuRL53Fs7H6CL4CNti3gGi+aY7S6lI2HNlAzVnKSO86sYtFOYsINoYfnCysHfZYUunid0t2M+6Z5ayvTOPmohIABvXbxtL/dwU3j07h+W8PIaZDMMt2HeX3K7/go70f4RQn9w+9v1mxJkQk8PiYxwF4bt1z5BY1vDZvLbtq54mky6lu4fiCclc5Kw6sYGnuUpbmLmVh9hKcUdvo1HlX7bbVB1fjHjDTSjj5GyCv4bpLtR3F1S7Ex2GqSqnGnfWjnTFGV904iyOlR7hnyT3kFOaQEpXC9wZ+j+k9phMSdOYEpxc3vQjArKJiEqOToUsfatyG/3hjHWuyrYusgrgBXF+ynRc7dyarZBO5RTlkdMwgMaYDc24YyAP/2MSft83FEWm4sfeNJEcln/E6ZzM1fSor8lbwcfbH/OyLn/HG1DcIdpzZtn+0uJLy1fNJAZ74JpmNzy3n3ssymDUq2eelGY0xPLryUT4/8HndxhjoEANzvwG+qdv81KVPMXP47fDVH2Ddq9D9zMXa9x+yEkSyG2spRKXUOfGlDHWwiDwoIu/ZtwdERCtu2fKK85i9eDY5hTk4xcn+4v08ufpJpr4/lTe2v1FvzP6mgk2szFtJuDi551SRdTUgwp+/yGZN9gk6R4byf98fy43TryfabZhurFr37+x6p/YY0wZ3Y9KQchyR2xETzPcG3NPi2H9+yc9JjEhk67GtzNsyr96+AyfKePxf27j2mQV0K8qk2gSxtcMo8k+V88SC7Yx7+nNeWpZF0WmLuzdk/p75fH7gcyKDI5mYPJGM8EuoLu5PF8cIJiZPZGLyRIbHW2saf5j1IYy8GxCr47j02BnHO7DfWjw+JSrFp45ppVTTfLnOfxkYAcy1byPsbQEv+1Q2sxfNJr8kn0GdB7H0O0t5ZsIz9OrUi4LyAp5f/zxXv381L2e+TGFlIb/f8HsA7qiEWLcb+kxh+8FCnv90FwDP3TSY0emxkGR9Cr75iDXadsHeBZRWl9a+bk1Hq9ul4vg4/vHVqRbHHx0SzZzxcxCEeVvmkVmQyZ4jxTzyz0yueH45f1uTy1j3JpzipqLbaD5/7HpeuW0Eg7vHcLy0iuc+2cW4//mcZxfv5FhJw52w+4v288y6ZwD4xZhf8MLEFwg9cTcVeXfw8OD/5oWJL/DCxBeYO3kuYUFhbCzYyMGQUGuCWE0VbDxzKcv9x63J6SndRrX4Z1dK1fElEYwyxsw2xnxu3+4CAv4/8Jvj33Dn4jspKC9gRNcRvHr1q8SGxXJtxrW8P/19/jjxjwzpMoTCykLmbp7LpHcnsbFgIzHBUcw+mAOh0VR0u4SH/5lJdY3h9jGpXNk33jp4VAJEJ9G7rJARsf0prS7lo73W+r+rD65mQ8E6wp2RVJ+4jLnLs1i376z99lCYb83addWfBzAqYRR3DrwTt3HzH4se4eo/LGD+JisBzRyWxG/6W6WeowZPx+EQpgxM4MMfjuPvd1/C2Iw4iitdzF2+l3FPf84TH26j2OsKweV28bMvfka5q5yp6VO5LuM6TpVVsSH3JE6HcHmfusmBEcERXJF8BQCLchbVLWW5/jWocdUFXFHI/kprsZrkjKt8/n0ppRrnSyKoEZEenm9EJIPmzSe46GQWZHL3J3dzsvIk45LG8fLkl4kIriupICJcnnw5f5v6N1675jXGdRtXO2zxPzoOJNIY6DmJZ5Zks/tICRldIvj5tf3qv0iS1VRyS5Q1NPLtnW/jNm5e3Gj1Mdwz+G5+MGEQbgMP/zOz3hvwGYyB9+6Chf8Ja+aesfuBoQ8QTgqVHCWi57MMGLyIv9+Xzv/eNIBoe3lGetdN2hIRxvfqzNv3jmH+/ZcyuV9XKl1u3lidy3df/ZqTpVayeXXLq2w5toWEiAQeu+QxAJbvOkqN2zA6PfaMmv7XplvzARbmLLQmpcVm1C5lWStrKQecVt9EShddHEap1uBLIvgvYJmILBeRFcDn+FAW4mL19aGvuXfJvRRXF3NV6lW8eOWLdHA2vG6tiDAqYRSvXPUK70x7hznj53D7UWsRlZ3R4/nrqn04HcILs4bRIeS0jle7eWhiSSnxHeLJLszm6bVPs+34NuLC4vhu3+/y0OTeDEyKJu9kOU8uOL2Wn5fdi+uWwLQXi/dWXimcyL4VV+EQggT2V6/g+8tm8cjiu9lhyqxx+nE9GjgwDE/pxJ9nj2TRjyeQEhvO1vxCZs1bzfJ96/jTlj8hCHPGzSEm1FoP4DN72Kj3pDiP8UnjiQ6JZvfJ3WQVZsMou/9jbV3/RdGuf3MyKIgwCaJLh4un3IhSbcmXMtRLgV7Ag8CPgD7GmGX+Dqw9WnFgBfd/dj/lrnJm9JjBs5c92+DIoIYMiBvAjKTLCcr9CiNBPLDOmkT28FW9GdQ95swn2Ikg+OAmvt3HGuf/9s63Afj+kO8THhxOiNPB72cNIyzYwfsb8/h4y6Ezj+N2w9JfW/fFAYX7YXf9uv+Lth2iujKGkRE/4qMbPuKm3jfhdDhZcmwTs5ISua9zNOsPr8c0UiIaoF9iNO/eN5Ze8ZHsLjjOj5f+hBpTw50D7mR04mgAqlxuVuyyigZO7hd/xjGCg4K5KtVq7lmYs7B2KUtyVsDRXVDj4sC+5QAkR3bXQnBKtZJGE4GITLS/zgSuA3rat+vsbQFlZd5KHlr2EFXuKmb1mcWvx/0aZ2MzbBuTtRTcLvaEDiCrJISRqZ247/KGP2mTOBQQOLyVmzKm175WUmQSN/Wqq03UMz6ytlnp5x9spaCoov5xtr0PBdshujtcaTXPeH/CBvgw8yAAM4Z0IyU6hSfGPsHimYuYXSl0cLtZVVnAXZ/cxR2L7mDFgRWNJoSu0WH88/tjScj4FLfzGI7qblzX/c7a/ev2naC40kXP+EhS4xquTnpdxnWAlQhMWAwMnmXH/Coc+JoD7nIAUjr5tvi8UursmroiuNz+Or2B2zQ/x9XuvLn9TVzGxW39buOxSx7DIS2YWGW3db9bPJDIUCe/mzWUIEcjn2rDoqFLH3BX07nwMNMyrFP+4LAHCT5tyOTtY1K5rHcXCsureW3VvrodNdWwbI51/4pHYdTd4OwA2cvg2B4AjhRVsCbnOCFOB9cMTKh9anxZIf95MJclR0u5f/B9xITGkHk0kwc+f4CbPrqJhdkLcbm9OnFtmce/pDRkFWKcFB/4Drf+eSM7DxcBdc1CkxtoFvIYHj+c+A7x5Jfks+XYltqlLNn8Nmx9l/32Sm4pUSmNHkMp1TyNvpsZY56w7/7KGHOX9w1rMZmAsrdwL2DVrGlRk0SNi5rdnwKw1D2cp2YMIDk2vOnn2M1D5G/gF2N+wXvT3+PajDMLrIkID0+2Skz8c91+KqrtPoCNb1pVQeN6wZBbrMXiB9vlJNb9GYCPNh/EGJjYJ75+5609mzim59X8YNgP+fTGT/mvkf9FfId4dp/czU+/+CnTP5jOu7vfpcquSHqs/BhPfvUkAA+NeJixyQM4VlLJrD+tIfPAKa9qo2c2C3kEOYKYkm51TC/MXghdB0DqOKgqgQ1/ZX+wlQiSo5s/iU4p1TBfPta+38C291o7kPassLKQY+XHCAsKo1tktxYdw5W7mqCKk2S7E+g7cDgzhyed/Undhllf8zcQGhRKn9g+jT50aHJHBneP4WRZNR9tPghVZbDiWWvnxMfq6gN5OmAz/wGVxSzYbDULXT/0tJ9rlz1Sxx4tFB4czh0D7mDRjYt4YuwTJEclk1eSx69W/4op70/h9W2v84tVv+Bk5UnGJo7lzoG38ZfZo5jcryuF5dXcPG81+0+UERsRwrCUxktlA7XJbvG+xdZVx+i6SXMHQq2KpC2ZTa2UalhTfQR9ReRGIEZEZnrd7gQarw98EcqxK12mx6S3rEkI2LHcWst3tXMUc741yLerCq8rgrMREe4YmwbAG6v3YdbOg5LDkDgE+l1f98DEwZAyFiqLOPrVm2zJKyQq1Fk3hwGg7AQcWGMtFNNzUr3XCQkK4abeN7HgWwt49rJn6d2pN0fLj/LbDb9lVf4qYkJj+M343+AQB2HBQbx823BmDOlGRbW1puQVfbo03hxm6x/bn7ToNE5UnGDtobXQdxpEJQKwP9QaoaVNQ0q1nqbe1fpg9QV0pH7/wHCg5XUNLkDZhdkAZHTMaPExog+utI5x6Uw6Rfg20oiuAyEoBI7vgfKzzyCeNjiRTuHB7M8/RM1KexWySb8Ex2m/ZvsTtmPdq4DhmoEJ9esG7VkCxg1p4yCsgRFNWDX+p6ZP5b3p7/HSpJcY2mUoToeTpy59ivjwuqQSHOTgd7OGctuYFJwO4dsjzv5JXkRq5xR8nPOxVUbi8p9Q5nByzFQT7Aima3jj/QxKqeZpdNiLMeZD4EMRGWuMWX0eY2p39p6y+gcyYlqWCExJAWmuHCpMMD2G+7Z6FwDOEEgYDPnr4eAm6HFlkw8PCw5i1qgUIla9hbOqEFLHQ49JZz6w73RMZAJxJTmMdezg+qGX1N9v9w80tiC8NxGpXe2s2l3dYOG6IIfwm28N4pfTBhDi9O2Kamr6VOZunsvS/Ut53PU4YSO/x4G0sfDxd+ge1Z0gh28F75RSZ+fLf+UmEfmhiMwVkdc8N79H1o54rgh6xDQy1PMsTu1YCsAm6UeXTg1/wm5UM5qHAG4f1IHvBVlv5CfHPtpwrX5nCAW9bwHgntDPGJvhtTCOq8oa5gpnXQLydA0lAW++JgGAtJg0BsQNoLS6lJV51tXU/lKr3IU2CynVunz5z/wb1nrC1wArsNYeLvZnUO1N9ikrEaR3bFlF7vJdVvnlnKiRzR9xVJsINvr28K1ziZBKPqsZxt/zExp93NuuiVSbIK4w63CWHKzbsf8rqCyCLv2gU1rzYm1l9UpOYBWwA+0oVqq1+ZIIehpjHgdKjTFvYE0uu+Qsz7lolFWXcbD0IE6Hs8VvQJH5XwJQ0m1c859cmwjWWzWDmnJqP6x/DYPwvGsWb329n+oa9xkPq3Eb3t5ZxSL3aBy4Yf1f63buspuFmnk14A9T0qcgCF/kfUFRVREHiu0F66P1ikCp1uRLIvBUMzslIgOBGKDxgeAXmZwia8RQalTqWZs+GnQih+iKg5wyEURnnLnIylnFZlgdtiVHoOhg049d/rRVunnQTVR17sfhogqW7DhyxsO+zjnOkaJKFofb8wI3vA6uSivR7PK9f8Df4sPjGZ0wmip3FUtzl9YlAm0aUqpV+ZII5olIJ+BxYAGwA3jWr1G1I55moRaPGMpZAcBqd396J3Zs/vMdDuhmVSLlYBPNQwU7rdm3Didy5c+ZbQ8lfXP1vjMeusAuKZExbDJ0HQRlx2D7v+DoTjiVC+Fx0H1k82P1A8+cgoU5C+uWqNREoFSr8qXo3J+NMSeNMSuMMRnGmHhjzCvnI7j2oHboaAtHDLn3LgdglXsgvbtGtSyIpjqMiw/Dp4/DnydZQz6H3wGxGcwcnkRESBBrsk+w63Bdl06lq4aFW63idNcPS6qbrLV2Xt3VQK9roJ2MypmUMolgRzBrD6/lcOlhnOIkMTKxrcNS6qLS6PBREXmkqScaY/639cNpfzxXBD06tmDEkNuNO3sFDmBv5AgiQ5tZpM6joURwIge+ehE2vQX2Wgf0mAQTrUXpo8KCuXFEd95cncubq/cx54ZBAKzcfYyiChf9EqPp1TUKOn0bljxu9UEUWQvStIf+AY+Y0BgmJE2oXe+4W2S35hf7U0o1qakrgij7NhL4AZBk3+7DmlQWEM7piuDINpwVJzhoYolIbLw8xFnZi9SQvwkOb4P374E/jLBX76q0Zt7e8zncPh/CY2ufdsfYVADmb8ynsNzq6vkw03qzry0pERIOw2637hcfsiaw9WjGXIfzYGpGXX+F1hhSqvU1NaHsKQARWQkMN8YU298/CXx8XqJrY1U1VRwoPoBDHKRGpzb/AHb/wKqagfRJjG55IFEJVhnpojx4xR55JEFWIblxD0F83waf1jM+inE941iVdZz3N+TxnVHJtRVApw/xqi006m5Y/RJgIG08hLawCctPLu9+OeHOcMpcZdo/oJQf+NJZ3BXwXui2yt520cstyqXG1JAUmUSYswXllbKXA1b/QJ+Ec0gEACljrK/OMKtw3IOb4IZXGk0CHrePSQPgb2ty+WTbYSqq3YxOiyWpo9eqarEZ0Psa637f684tTj/o4OzA5NTJAPTsqOsQKNXafGlsfRNYKyIf2N9/C3jdbxG1I+c0o9hVBblfAbDKPYD7E87xU/Y1cyDjCusNO9L30buT+8XTLSaMnGOlPLN4JwAzTq80CjDjj1ZpiSHfPbc4/eSno3/KiK4jmJ4xva1DUeqi48uooTnAXcBJ+3aXMeZ//B1Ye3BOM4rz10N1GbvdSZwKiiW9c8MrcvksKgGG396sJADgDHJw6xirWauguBKnQ7h2UAOjbiK7WCOOgtpnR2x0SDQze808Y1EepdS5a6oMdbT9NRbYh1Vq4m9Arr3toud9RbBz7ad8/dLdlBed9PHJywGrWahHl0iCg1pWvro13DwqmRD79Sf06kysr9VPlVIBoal3p3/YXzcA671unu8vep5VyTJiMqj87L+55Oh77Jv/S9+enG13FLsH0vdcm4XOUVxkKDeO6A7ArFE66kYpVV9To4am2V9bVmntAudyu8gtzAWgW0QqZZV5INAz920ofBRimlhhrLIY8tfjxsHX7n7cf64dxa3gyRn9uW1MCgO6NbP6qVLqotfUhLIm5woYY3wrh3mByi/Jp8pdRdfwruw7VMpQjgEQbKoxK55BZrzY+JNzvwK3i6zgvhRXhLf5FQFAqDNIk4BSqkFN9Qz+tol9Bmhfs45amfeM4t27djBCDEUmnHAqCNr0d7j0QejcyFBGu39gRXV/APq0g0SglFKNaappqOnlsC5y3v0DBet3ALBDepDj6swtsgyWzYFv/7XhJ9v9A0ur+hMV5iQxJqCWeFZKXWB8GsoiIgNF5Dsicofn5u/A2ppnwfq06HQqCrIAiEzszQuumVQRDNvnw6HNZz6xpAAKtlMTFMZGdy/6JkQ1fzEapZQ6j86aCETkCeAP9u1KrBLUM/wcV5vzrFMcahLp6rLKNqf2HsRh4njLfbX1oKW/PvOJOdayioeih1JFsDYLKaXaPV+uCG4CJgGHjTF3AUOwFqe5aBljaucQHD0RQ5pY9XmiEnvTp2sUL1ZNp8YZAVlLamcP17L7BzKDhwKce2kJpZTyM18SQbkxxg247ElmBcBFPRj9cOlhyl3lxIbFsiPPTaoctnbEZnBpzzhOEs3aBGvxdz57qm4JSWNq+weWVFo1gNrDiCGllGqKL4lgvYh0BF7Fmky2EVjt16jamOdqID0mnY05BXQXa+gondIY37MzAC9VToEOsXBgDez51Np/MgcK92M6dOLT41YpiN7xmgiUUu1bUyUmXhKRccaY+40xp+xVya4CZttNRGclIlNEZJeIZInIow3sv09EtopIpoh8KSL9W/6jtB5P/0Bih1RMYR7BUoOJToLgDoxOjyXIIazOr6Zi7EPWE5b+Gtzu2mah0m5jKXdBYkwYMeFaG0cp1b41dUWwG3heRPaJyLMiMswYs88Ys8WXA4tIEPASMBXoD9zSwBv9P4wxg4wxQ7E6odvFqmeeKwKqu5JmNwtJrLUwTVRYMEO6x1DjNqyOvQGik+DIVmsUkd0slBs1CtD5A0qpC0OjicAY84IxZixwOXAceE1EdorIEyLS24djjwayjDHZxpgq4B3g+tNeo8jr2wisiWptzpMITp3qRIoUWBtj6ypteJqHVuYUw+U/tTYum1M7YmitYzCgiUApdWHwpQx1rjHmGWPMMOAWrPUIvvHh2EnAAa/v8+xt9YjID0VkL9YVwYM+Re1HxpjapqG9ByNrrwiIrVuq8lI7EXyVdRyG3gpxPeFENpSfgOjufH2qI6AdxUqpC4Mv8wicIjJdRN4CFgG7gJmtFYAx5iVjTA/gp8AvGonhXhFZLyLrjx492lov3aDjFccpqioiIjiSrENCusMaOuqdCIaldKRDcBC7jhRTUOaCKx+rO0DGFewqKAGgT1cdOqqUav+a6iy+SkRew/okfw/WOsU9jDE3G2M+9OHY+dQfZtrd3taYd7CuNs5gjJlnjBlpjBnZpUsXH1665TwziruEJuM2Qu9gO/F4JYJQZxCj0q0lGVbvPQ79vwUJVnNQVdoV7DteSpBD6BF/jovRKKXUedDUFcHPgK+AfsaYGcaYfxhjSptx7HVALxFJF5EQ4GZggfcDRKSX17fXAXuacXy/8DQLOWsScOAm0W03DXWqX417XI84AFZlHQOHA255G2b8gZ1xkzAGMjpHEOoMOq+xK6VUSzRVdO6cqosaY1wi8gDwCRAEvGaM2S4ivwLWG2MWAA+IyGSgGmsZzNnn8pqtwdNRXFwcSwIncJpqiOwKoZH1HjfO7idYlXUcYwwS0x2G38HO9Va3iHYUK6UuFH5doNYYsxBYeNq2X3rd/7E/X78lPOWn8wuiGdpA/4BH/8RoOoUHk3+qnNzjZaTZaxLvOlwMaEexUurC0XYL6bZTniuCirLOjIqy1yduIBE4HMKlPeyrgr3Hard7EoHWGFJKXSg0EXgpqiriaPlRnBKCqe7E8NpE0PCsyLEyAAAN0UlEQVRqnZf29OonsO3UKwKl1AXGr01DFxpPs1CoSQAc9HR6JpOdeUUAdRPLvtp7HLfbcLKsimMllUSEBJHUscP5CFkppc6ZJgIvnqGj5aXWG3x8tbUOQWOJICU2nKSOHcg/Vc6OQ0UUlVcD0DshCodDF6NRSl0YtGnIi2foaHlpHJ0jQggu3Gft6NRw05CIMM6reUibhZRSFyJNBF48HcXuqq5MTnYjrnIIj4MOHRt9Tu0w0r3H6zqKu2oiUEpdODQReKlNBJXxTIiz3tQbaxby8IwcWptznK35hYDVNKSUUhcKTQS2suoyDpYcBOPAXRXH4PDj1o5GmoU8ukSF0jchiopqNzsOWcVU++rQUaXUBUQTgW1f0T4MhpqqzoSHhJDkPmTtOMsVAdRdFYCVGGIjQvwVplJKtTpNBDbvZqFhKR1xnLRGEPmSCMb3iqu9rx3FSqkLjSYCm2cOgbsqnpGpsdb6AuBTIhidHofTHi6qHcVKqQuNJgLbnlNW4VN3ZTyjUjvBCd+vCCJDnQxNthejSdT+AaXUhUUnlGGtSra5YLP1TWUywzq7oKoYQmMgPNanYzx2XT8+zDzItMGJfoxUKaVanyYCILcol5OVJ3G7IunbOZ2Ikv3Wjth0EN9mCA9L6cSwlE5+jFIppfxDm4aATQWbAKgpS2VUWvP6B5RS6kKniQDYWLARgJryNCsRNGPEkFJKXeg0EQDrD1uJgIp0xvXorFcESqmAEvCJ4Fj5MfJK9mPcwQxP6E9MeLAmAqVUQAn4RJBZkAlATXkKV/VLsjZqIlBKBZCATwTrD28AoKYsjUn9ukLZCSg/CcEREBnfxtEppZT/BXwiWJW3HoD4kL6kd46o31Hs49BRpZS6kAV0IiirLiO3ZA/GCFdljLY21s4obrrqqFJKXSwCOhFsO7YNQw3uim5M6Z9qbdT+AaVUgAnoRPBZzhoAgqozGJFqzwrWRKCUCjABnQhW5a0DYFDcEJxB9qnQRKCUCjABmwhcbhd5Zd8AMK3PpXU7NBEopQJMwCaCjYd24JZK3FWxTBvQ19pYUQSlR8EZBlFaRVQpFRgCNhHM3/ElAHHOPkSHBVsbPUNHO6WDI2BPjVIqwATsu93aQ9ZEshFdh9dt1GYhpVQACshEUOWq4Wj1TgBu7D+hbkdtItA5BEqpwBGQiWDxzh3gLELc4Vya2q9uh14RKKUCUEAmgg93rgKgW1g/HOJ1CnRWsVIqAAVcIjDGsPmYtSLZpUkj6+/UKwKlVAAKuESQVVBCmSMLgOt6j6vbUVUKxYfAEQzR3dsoOqWUOv8CLhH8e1sWQaEFOAhmUJcBdTtO7rO+dkqFIGebxKaUUm0h4BLBJ3ut+kKpkX0ICQqp26HNQkqpABVQieBEaRU5JdsBmJA86rSdmgiUUoEpoBLBsp0FODrsA+CSbl4dxdXlsN+6UtBEoJQKNAHVGL7kmwMEheUBwpAuQ6CiENb9BdbMtWoMAXQd0OQxlFLqYuPXKwIRmSIiu0QkS0QebWD/IyKyQ0S2iMhSEUn1VyxVLjdf7t+MOGpIj0wl5ssX4HeDYOlTVhJIHAKz/g5p4/0VglJKtUt+uyIQkSDgJeAqIA9YJyILjDE7vB62CRhpjCkTkR8AzwKz/BHP1znHCQ3eQhUw+vA3sHWltSN1PEx4BHpM1DWKlVIByZ9NQ6OBLGNMNoCIvANcD9QmAmPMMq/HrwFu80skxhC++BFGR6zhS8IYVlYKvafA+Ecg5RK/vKRSSl0o/JkIkoADXt/nAU29694NLGpoh4jcC9wLkJKS0vxIRDhVUszmrtZw0eE3vQ3pVzT/OEopdRFqF6OGROQ2YCTwXEP7jTHzjDEjjTEju3Tp0qLX6H7LwxQHOUgITyBRk4BSStXy5xVBPpDs9X13e1s9IjIZeAy43BhT6a9gNlceAWBY12H+egmllLog+fOKYB3QS0TSRSQEuBlY4P0AERkG/AmYYYwp8GMsbCzYCMDw+OFneaRSSgUWvyUCY4wLeAD4BPgG+D9jzHYR+ZWIzLAf9hwQCbwrIpkisqCRw50zt3ET4ghhWLxeESillDcxxrR1DM0ycuRIs379+hY9t6qmCqfDWX8NAqWUCgAissEYM7KhfQE1s7hekTmllFJAOxk1pJRSqu1oIlBKqQCniUAppQKcJgKllApwmgiUUirAaSJQSqkAp4lAKaUC3AU3oUxEjgK5LXx6Z+BYK4bT2jS+c6Pxnbv2HqPG13KpxpgGq3ZecIngXIjI+sZm1rUHGt+50fjOXXuPUePzD20aUkqpAKeJQCmlAlygJYJ5bR3AWWh850bjO3ftPUaNzw8Cqo9AKaXUmQLtikAppdRpNBEopVSAC5hEICJTRGSXiGSJyKNtFEOyiCwTkR0isl1Efmxvf1JE8u1V2jJF5Fqv5/zMjnmXiFxzHmLcJyJb7TjW29tiRWSJiOyxv3ayt4uIvGjHt0VE/LoOqIj08TpHmSJSJCIPteX5E5HXRKRARLZ5bWv2+RKR2fbj94jIbD/H95yI7LRj+EBEOtrb00Sk3Os8vuL1nBH230WW/TOIH+Nr9u/TX//fjcT3T6/Y9olIpr39vJ+/VmOMuehvQBCwF8gAQoDNQP82iCMRGG7fjwJ2A/2BJ4H/bODx/e1YQ4F0+2cI8nOM+4DOp217FnjUvv8o8Ix9/1pgESDAGODr8/w7PQyktuX5Ay4DhgPbWnq+gFgg2/7ayb7fyY/xXQ047fvPeMWX5v24046z1o5Z7J9hqh/ja9bv05//3w3Fd9r+3wK/bKvz11q3QLkiGA1kGWOyjTFVwDvA9ec7CGPMIWPMRvt+MdZazklNPOV64B1jTKUxJgfIwvpZzrfrgTfs+28A3/La/qaxrAE6ikjieYppErDXGNPULHO/nz9jzErgRAOv25zzdQ2wxBhzwhhzElgCTPFXfMaYT421pjjAGqB7U8ewY4w2xqwx1rvam14/U6vH14TGfp9++/9uKj77U/13gLebOoY/z19rCZREkAQc8Po+j6bfgP1ORNKAYcDX9qYH7Ev11zxNCbRN3Ab4VEQ2iMi99rauxphD9v3DQNc2jM/jZur/A7aX8wfNP19teR6/h/UJ1SNdRDaJyAoRmWBvS7JjOp/xNef32VbnbwJwxBizx2tbezl/zRIoiaBdEZFI4H3gIWNMEfAy0AMYChzCutxsK+ONMcOBqcAPReQy7532J5o2HXMsIiHADOBde1N7On/1tIfz1RgReQxwAW/Zmw4BKcaYYcAjwD9EJLoNQmu3v8/T3EL9DyPt5fw1W6Akgnwg2ev77va2805EgrGSwFvGmPkAxpgjxpgaY4wbeJW65ovzHrcxJt/+WgB8YMdyxNPkY38taKv4bFOBjcaYI3as7eb82Zp7vs57nCJyJzANuNVOVthNLsft+xuw2t1727F4Nx/5Nb4W/D7b4vw5gZnAP73ibhfnryUCJRGsA3qJSLr9afJmYMH5DsJuU/wL8I0x5n+9tnu3q98AeEYoLABuFpFQEUkHemF1OvkrvggRifLcx+pU3GbH4RnJMhv40Cu+O+zRMGOAQq8mEX+q90msvZw/L809X58AV4tIJ7sZ5Gp7m1+IyBTgJ8AMY0yZ1/YuIhJk38/AOl/ZdoxFIjLG/hu+w+tn8kd8zf19tsX/92RgpzGmtsmnvZy/Fmnr3urzdcMasbEbK0s/1kYxjMdqJtgCZNq3a4G/AVvt7QuARK/nPGbHvAs/jzTAGnWx2b5t95wnIA5YCuwBPgNi7e0CvGTHtxUYeR7OYQRwHIjx2tZm5w8rIR0CqrHafu9uyfnCaqvPsm93+Tm+LKw2dc/f4Cv2Y2+0f++ZwEZgutdxRmK9Ie8F/ohdlcBP8TX79+mv/++G4rO3vw7cd9pjz/v5a62blphQSqkAFyhNQ0oppRqhiUAppQKcJgKllApwmgiUUirAaSJQSqkAp4lABRwRKbG/ponId1v52D8/7fuvWvP4SvmDJgIVyNKAZiUCe0ZpU+olAmPMpc2MSanzThOBCmRPAxPs2vEPi0iQWLX619kFz74PICJXiMgXIrIA2GFv+5ddmG+7pzifiDwNdLCP95a9zXP1Ifaxt9l16Wd5HXu5iLwn1hoBb3lq1YvI02KtXbFFRJ4/72dHBYyzfbpR6mL2KFbd+2kA9ht6oTFmlIiEAqtE5FP7scOBgcYqfwzwPWPMCRHpAKwTkfeNMY+KyAPGmKENvNZMrCJqQ4DO9nNW2vuGAQOAg8AqYJyIfINVXqGvMcaIvXiMUv6gVwRK1bkaqxZQJlZ58DisejEAa72SAMCDIrIZq55/stfjGjMeeNtYxdSOACuAUV7HzjNWkbVMrCarQqAC+IuIzATKGjimUq1CE4FSdQT4kTFmqH1LN8Z4rghKax8kcgVW0bGxxpghwCYg7Bxet9Lrfg3W6mEurKqb72FVCV18DsdXqkmaCFQgK8ZaMtTjE+AHdqlwRKS3XYX1dDHASWNMmYj0xVqC0KPa8/zTfAHMsvshumAtgdhoJVR7zYoYY8xC4GGsJiWl/EL7CFQg2wLU2E08rwMvYDXLbLQ7bI/S8JKCi4H77Hb8XVjNQx7zgC0istEYc6vX9g+AsViVXQ3wE2PMYTuRNCQK+FBEwrCuVB5p2Y+o1Nlp9VGllApw2jSklFIBThOBUkoFOE0ESikV4DQRKKVUgNNEoJRSAU4TgVJKBThNBEopFeD+P7taP6RGFxAzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdx7JVRGNbKv",
        "outputId": "c8bb3db9-9227-493a-b23b-b0057aeb728d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for l in ['0.04', '0.4', '4.0']:\n",
        "  test_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=float(l), num_inner_updates=1, meta_train=False, learn_inner_update_lr=True,\n",
        "                      model_file='/content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_{:s}.learn_inner_update_lr_True'.format(l))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.04.learn_inner_update_lr_True\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.5133333, 0.1683911, 0.01347409371487948)\n",
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_True\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.542, 0.17842646, 0.014277090219152081)\n",
            "Restoring model weights from  /content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_4.0.learn_inner_update_lr_True\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.57299995, 0.18404077, 0.014726328621489239)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auOauf7l7fSo"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmGfgV-geFj"
      },
      "source": [
        "# models/ProtoNet\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class ProtoNet(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_filters, latent_dim):\n",
        "    super(ProtoNet, self).__init__()\n",
        "    self.num_filters = num_filters\n",
        "    self.latent_dim = latent_dim\n",
        "    num_filter_list = self.num_filters + [latent_dim]\n",
        "    self.convs = []\n",
        "    for i, num_filter in enumerate(num_filter_list):\n",
        "      block_parts = [\n",
        "        layers.Conv2D(\n",
        "          filters=num_filter,\n",
        "          kernel_size=3,\n",
        "          padding='SAME',\n",
        "          activation='linear'),\n",
        "      ]\n",
        "\n",
        "      block_parts += [layers.BatchNormalization()]\n",
        "      block_parts += [layers.Activation('relu')]\n",
        "      block_parts += [layers.MaxPool2D()]\n",
        "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
        "      self.__setattr__(\"conv%d\" % i, block)\n",
        "      self.convs.append(block)\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "  def call(self, inp):\n",
        "    out = inp\n",
        "    for conv in self.convs:\n",
        "      out = conv(out)\n",
        "    out = self.flatten(out)\n",
        "    return out\n",
        "\n",
        "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
        "  \"\"\"\n",
        "    calculates the prototype network loss using the latent representation of x\n",
        "    and the latent representation of the query set\n",
        "    Args:\n",
        "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
        "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
        "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
        "      num_classes: number of classes (N) for classification\n",
        "      num_support: number of examples (S) in the support set\n",
        "      num_queries: number of examples (Q) in the query set\n",
        "    Returns:\n",
        "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
        "      acc: the accuracy of classification on the queries\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "\n",
        "  # compute the prototypes\n",
        "  # compute the distance from the prototypes\n",
        "  # compute cross entropy loss\n",
        "  # note - additional steps are needed!\n",
        "  # return the cross-entropy loss and accuracy\n",
        "\n",
        "  # compute the prototypes\n",
        "  x_latent = tf.reshape(x_latent, [num_classes, num_support, -1]) # [N, S, D]\n",
        "  x_latent = tf.reduce_mean(x_latent, axis=1) # take mean over support examples, to get [N, D] \n",
        "  x_latents = tf.tile(tf.reshape(x_latent, [1, num_classes, -1]), [num_classes*num_queries, 1, 1]) # [N*Q, N, D]\n",
        "  q_latent = tf.reshape(q_latent, [num_classes * num_queries, 1, -1]) # [N*Q, 1, D]\n",
        "  q_latent = tf.tile(q_latent, [1, num_classes, 1]) # [N*Q, N, D]\n",
        "  # compute the distance from the prototypes\n",
        "  distances = tf.reduce_sum((q_latent - x_latents)**2.0, axis=-1) # [N*Q, N]\n",
        "  # compute accuracy\n",
        "  q_labels = tf.reshape(labels_onehot, shape=(num_classes*num_queries, -1)) # [N*Q, N]\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(q_labels, axis=-1), tf.argmin(distances, axis=-1)), tf.float32)) # [N*Q,] --> scalar\n",
        "  # compute cross entropy loss\n",
        "  ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=q_labels, logits=-distances)) # [N*Q,] --> scalar\n",
        "  #############################\n",
        "  return ce_loss, acc"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_bOml4PhkSM"
      },
      "source": [
        "# run_ProtoNet\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def proto_net_train_step(model, optim, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    x_latent = model(x)\n",
        "    q_latent = model(q)\n",
        "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return ce_loss, acc\n",
        "\n",
        "def proto_net_eval(model, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  x_latent = model(x)\n",
        "  q_latent = model(q)\n",
        "  ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  return ce_loss, acc \n",
        "\n",
        "def run_protonet(data_path='./omniglot_resized', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, k_meta_test_shot=5, n_meta_test_query=5):\n",
        "  n_epochs = 20\n",
        "  n_episodes = 100\n",
        "\n",
        "  im_width, im_height, channels = 28, 28, 1\n",
        "  num_filters = 32\n",
        "  latent_dim = 16\n",
        "  num_conv_layers = 3\n",
        "  n_meta_test_episodes = 1000\n",
        "\n",
        "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  val_acc = []\n",
        "  # call DataGenerator with k_shot+n_query samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)\n",
        "  for ep in range(n_epochs):\n",
        "    for epi in range(n_episodes):\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "\n",
        "      # sample a batch of training data and partition it into\n",
        "      # support and query sets\n",
        "      batched_input, batched_label = data_generator.sample_batch('meta_train', batch_size=1, shuffle=False, swap=False)\n",
        "      split_input = np.split(batched_input.squeeze(0), [k_shot], axis=1) # [N, S, 784]\n",
        "      support, query = split_input[0], split_input[1]\n",
        "      split_label = np.split(batched_label.squeeze(0), [k_shot], axis=1) # [N, Q, N]\n",
        "      _, labels = split_label[0], split_label[1]\n",
        "      support = support.reshape([n_way, -1, im_width, im_height, channels]) # [N, S, 28, 28, 1]\n",
        "      query = query.reshape([n_way, -1, im_width, im_height, channels]) # [N, Q, 28, 28, 1]\n",
        "      labels = labels.reshape([n_way, -1, n_way]) # [N, Q, N]\n",
        "      # Shuffle label assignments of query set\n",
        "      query_shuffle_i = np.arange(n_way)\n",
        "      np.random.shuffle(query_shuffle_i)\n",
        "      query = query[query_shuffle_i, :, :, :, :]\n",
        "      labels = labels[query_shuffle_i, :, :]\n",
        "      #############################\n",
        "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels)\n",
        "      if (epi+1) % 50 == 0:\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "\n",
        "        # sample a batch of validation data and partition it into\n",
        "        # support and query sets\n",
        "        batched_input, batched_label = data_generator.sample_batch('meta_validation', batch_size=1, shuffle=False, swap=False)\n",
        "        split_input = np.split(batched_input.squeeze(0), [k_shot], axis=1) # [N, S, 784]\n",
        "        support, query = split_input[0], split_input[1]\n",
        "        split_label = np.split(batched_label.squeeze(0), [k_shot], axis=1) # [N, Q, N]\n",
        "        _, labels = split_label[0], split_label[1]\n",
        "        support = support.reshape([n_way, -1, im_width, im_height, channels]) # [N, S, 28, 28, 1]\n",
        "        query = query.reshape([n_way, -1, im_width, im_height, channels]) # [N, Q, 28, 28, 1]\n",
        "        labels = labels.reshape([n_way, -1, n_way]) # [N, Q, N]\n",
        "        # Shuffle label assignments of query set\n",
        "        query_shuffle_i = np.arange(n_way)\n",
        "        np.random.shuffle(query_shuffle_i)\n",
        "        query = query[query_shuffle_i, :, :, :, :]\n",
        "        labels = labels[query_shuffle_i, :, :]\n",
        "        #############################\n",
        "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "        val_acc.append(val_ac)\n",
        "        print('[epoch {}/{}, episode {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
        "                                                                    n_epochs,\n",
        "                                                                    epi+1,\n",
        "                                                                    n_episodes,\n",
        "                                                                    ls,\n",
        "                                                                    ac,\n",
        "                                                                    val_ls,\n",
        "                                                                    val_ac))\n",
        "\n",
        "  print('Testing...')\n",
        "  meta_test_accuracies = []\n",
        "  for epi in range(n_meta_test_episodes):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # support and query sets\n",
        "    batched_input, batched_label = data_generator.sample_batch('meta_test', batch_size=1, shuffle=False, swap=False)\n",
        "    split_input = np.split(batched_input.squeeze(0), [k_shot], axis=1) # [N, S, 784]\n",
        "    support, query = split_input[0], split_input[1]\n",
        "    split_label = np.split(batched_label.squeeze(0), [k_shot], axis=1) # [N, Q, N]\n",
        "    _, labels = split_label[0], split_label[1]\n",
        "    support = support.reshape([n_meta_test_way, -1, im_width, im_height, channels]) # [N, S, 28, 28, 1]\n",
        "    query = query.reshape([n_meta_test_way, -1, im_width, im_height, channels]) # [N, Q, 28, 28, 1]\n",
        "    labels = labels.reshape([n_meta_test_way, -1, n_meta_test_way]) # [N, Q, N]\n",
        "    # Shuffle label assignments of query set\n",
        "    query_shuffle_i = np.arange(n_meta_test_way)\n",
        "    np.random.shuffle(query_shuffle_i)\n",
        "    query = query[query_shuffle_i, :, :, :, :]\n",
        "    labels = labels[query_shuffle_i, :, :]\n",
        "    #############################\n",
        "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "    meta_test_accuracies.append(ac)\n",
        "    if (epi+1) % 50 == 0:\n",
        "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
        "  avg_acc = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
        "  return val_acc"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kead1vdk7Uz"
      },
      "source": [
        "## Prob 3.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Tv12fbTQqJ",
        "outputId": "36c5e698-530f-4149-b5bb-35867cabdaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_acc = run_protonet('./omniglot_resized/')\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/proto_prob2.npy', np.array(val_acc))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1/20, episode 50/100] => meta-training loss: 1.49388, meta-training acc: 0.53000, meta-val loss: 1.63576, meta-val acc: 0.50556\n",
            "[epoch 1/20, episode 100/100] => meta-training loss: 1.52083, meta-training acc: 0.48000, meta-val loss: 1.80917, meta-val acc: 0.61111\n",
            "[epoch 2/20, episode 50/100] => meta-training loss: 1.50433, meta-training acc: 0.57000, meta-val loss: 1.52165, meta-val acc: 0.52222\n",
            "[epoch 2/20, episode 100/100] => meta-training loss: 1.42447, meta-training acc: 0.53000, meta-val loss: 0.78371, meta-val acc: 0.80556\n",
            "[epoch 3/20, episode 50/100] => meta-training loss: 1.11666, meta-training acc: 0.62000, meta-val loss: 1.03477, meta-val acc: 0.68333\n",
            "[epoch 3/20, episode 100/100] => meta-training loss: 1.12166, meta-training acc: 0.68000, meta-val loss: 0.64426, meta-val acc: 0.80556\n",
            "[epoch 4/20, episode 50/100] => meta-training loss: 0.57384, meta-training acc: 0.78000, meta-val loss: 0.56199, meta-val acc: 0.80556\n",
            "[epoch 4/20, episode 100/100] => meta-training loss: 0.64296, meta-training acc: 0.82000, meta-val loss: 1.31533, meta-val acc: 0.66667\n",
            "[epoch 5/20, episode 50/100] => meta-training loss: 0.61961, meta-training acc: 0.82000, meta-val loss: 0.56886, meta-val acc: 0.86667\n",
            "[epoch 5/20, episode 100/100] => meta-training loss: 0.62016, meta-training acc: 0.80000, meta-val loss: 0.38851, meta-val acc: 0.86667\n",
            "[epoch 6/20, episode 50/100] => meta-training loss: 0.75098, meta-training acc: 0.77000, meta-val loss: 0.90868, meta-val acc: 0.72222\n",
            "[epoch 6/20, episode 100/100] => meta-training loss: 2.03686, meta-training acc: 0.67000, meta-val loss: 0.77119, meta-val acc: 0.76111\n",
            "[epoch 7/20, episode 50/100] => meta-training loss: 0.87716, meta-training acc: 0.71000, meta-val loss: 1.11494, meta-val acc: 0.61111\n",
            "[epoch 7/20, episode 100/100] => meta-training loss: 1.36562, meta-training acc: 0.71000, meta-val loss: 0.44593, meta-val acc: 0.85556\n",
            "[epoch 8/20, episode 50/100] => meta-training loss: 0.61054, meta-training acc: 0.80000, meta-val loss: 0.68591, meta-val acc: 0.81667\n",
            "[epoch 8/20, episode 100/100] => meta-training loss: 0.86195, meta-training acc: 0.86000, meta-val loss: 0.55132, meta-val acc: 0.83333\n",
            "[epoch 9/20, episode 50/100] => meta-training loss: 0.27829, meta-training acc: 0.91000, meta-val loss: 0.74219, meta-val acc: 0.77778\n",
            "[epoch 9/20, episode 100/100] => meta-training loss: 0.57213, meta-training acc: 0.77000, meta-val loss: 0.58510, meta-val acc: 0.82222\n",
            "[epoch 10/20, episode 50/100] => meta-training loss: 0.59538, meta-training acc: 0.72000, meta-val loss: 0.66665, meta-val acc: 0.77778\n",
            "[epoch 10/20, episode 100/100] => meta-training loss: 0.37643, meta-training acc: 0.87000, meta-val loss: 0.47288, meta-val acc: 0.86667\n",
            "[epoch 11/20, episode 50/100] => meta-training loss: 0.26885, meta-training acc: 0.93000, meta-val loss: 0.69782, meta-val acc: 0.77222\n",
            "[epoch 11/20, episode 100/100] => meta-training loss: 0.52745, meta-training acc: 0.81000, meta-val loss: 1.62103, meta-val acc: 0.73889\n",
            "[epoch 12/20, episode 50/100] => meta-training loss: 0.39749, meta-training acc: 0.87000, meta-val loss: 0.71929, meta-val acc: 0.76111\n",
            "[epoch 12/20, episode 100/100] => meta-training loss: 0.24254, meta-training acc: 0.90000, meta-val loss: 0.40122, meta-val acc: 0.85556\n",
            "[epoch 13/20, episode 50/100] => meta-training loss: 0.69863, meta-training acc: 0.83000, meta-val loss: 0.74203, meta-val acc: 0.76667\n",
            "[epoch 13/20, episode 100/100] => meta-training loss: 0.48703, meta-training acc: 0.87000, meta-val loss: 0.35998, meta-val acc: 0.90556\n",
            "[epoch 14/20, episode 50/100] => meta-training loss: 0.42754, meta-training acc: 0.83000, meta-val loss: 0.31087, meta-val acc: 0.86667\n",
            "[epoch 14/20, episode 100/100] => meta-training loss: 0.21733, meta-training acc: 0.92000, meta-val loss: 0.54543, meta-val acc: 0.80000\n",
            "[epoch 15/20, episode 50/100] => meta-training loss: 0.38373, meta-training acc: 0.86000, meta-val loss: 0.62327, meta-val acc: 0.81667\n",
            "[epoch 15/20, episode 100/100] => meta-training loss: 0.47052, meta-training acc: 0.85000, meta-val loss: 0.27256, meta-val acc: 0.91667\n",
            "[epoch 16/20, episode 50/100] => meta-training loss: 0.40146, meta-training acc: 0.84000, meta-val loss: 0.41631, meta-val acc: 0.86111\n",
            "[epoch 16/20, episode 100/100] => meta-training loss: 0.30158, meta-training acc: 0.91000, meta-val loss: 0.48890, meta-val acc: 0.82778\n",
            "[epoch 17/20, episode 50/100] => meta-training loss: 0.52277, meta-training acc: 0.83000, meta-val loss: 0.47425, meta-val acc: 0.84444\n",
            "[epoch 17/20, episode 100/100] => meta-training loss: 0.53871, meta-training acc: 0.78000, meta-val loss: 0.53644, meta-val acc: 0.81111\n",
            "[epoch 18/20, episode 50/100] => meta-training loss: 0.22597, meta-training acc: 0.93000, meta-val loss: 0.43522, meta-val acc: 0.88889\n",
            "[epoch 18/20, episode 100/100] => meta-training loss: 0.40001, meta-training acc: 0.88000, meta-val loss: 0.57145, meta-val acc: 0.82222\n",
            "[epoch 19/20, episode 50/100] => meta-training loss: 0.65543, meta-training acc: 0.76000, meta-val loss: 0.52057, meta-val acc: 0.84444\n",
            "[epoch 19/20, episode 100/100] => meta-training loss: 0.20052, meta-training acc: 0.94000, meta-val loss: 0.20204, meta-val acc: 0.92222\n",
            "[epoch 20/20, episode 50/100] => meta-training loss: 0.38872, meta-training acc: 0.88000, meta-val loss: 0.69945, meta-val acc: 0.78333\n",
            "[epoch 20/20, episode 100/100] => meta-training loss: 0.49536, meta-training acc: 0.83000, meta-val loss: 0.38307, meta-val acc: 0.89444\n",
            "Testing...\n",
            "[meta-test episode 50/1000] => loss: 0.35137, acc: 0.92778\n",
            "[meta-test episode 100/1000] => loss: 0.29526, acc: 0.90000\n",
            "[meta-test episode 150/1000] => loss: 0.49478, acc: 0.85000\n",
            "[meta-test episode 200/1000] => loss: 0.24054, acc: 0.94444\n",
            "[meta-test episode 250/1000] => loss: 0.72430, acc: 0.80000\n",
            "[meta-test episode 300/1000] => loss: 0.68136, acc: 0.82222\n",
            "[meta-test episode 350/1000] => loss: 0.50975, acc: 0.81667\n",
            "[meta-test episode 400/1000] => loss: 0.34243, acc: 0.92778\n",
            "[meta-test episode 450/1000] => loss: 0.39226, acc: 0.84444\n",
            "[meta-test episode 500/1000] => loss: 0.58579, acc: 0.78333\n",
            "[meta-test episode 550/1000] => loss: 0.44510, acc: 0.85000\n",
            "[meta-test episode 600/1000] => loss: 0.80620, acc: 0.77222\n",
            "[meta-test episode 650/1000] => loss: 0.32778, acc: 0.87222\n",
            "[meta-test episode 700/1000] => loss: 0.29031, acc: 0.92778\n",
            "[meta-test episode 750/1000] => loss: 0.51347, acc: 0.78333\n",
            "[meta-test episode 800/1000] => loss: 0.38551, acc: 0.86111\n",
            "[meta-test episode 850/1000] => loss: 0.77571, acc: 0.76111\n",
            "[meta-test episode 900/1000] => loss: 0.79521, acc: 0.74444\n",
            "[meta-test episode 950/1000] => loss: 0.47313, acc: 0.80556\n",
            "[meta-test episode 1000/1000] => loss: 0.32873, acc: 0.86111\n",
            "Average Meta-Test Accuracy: 0.85058, Meta-Test Accuracy Std: 0.04941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwtqwYOslYtG",
        "outputId": "4bc4fff0-511d-4e63-e6cd-f2a2a2929755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.close('all')\n",
        "\n",
        "proto_val = np.load('/content/gdrive/My Drive/cs330_hw2/proto_prob2.npy')\n",
        "plt.plot(np.arange(len(proto_val))*0.5, proto_val, linewidth=2, linestyle='solid', color='k')\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "#plt.legend()\n",
        "plt.savefig('/content/gdrive/My Drive/cs330_hw2/prob_2.3.png')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJEMuFC8wRUA"
      },
      "source": [
        "# Prob 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxKAVI63S3r",
        "outputId": "c2e46cca-a982-4c10-de1f-38fcf90e28cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train MAML\n",
        "val_acc = run_maml(n_way=5, k_shot=1, inner_update_lr=0.4, num_inner_updates=1, logdir='/content/gdrive/My Drive/cs330_hw2')\n",
        "val_acc = np.array([v.numpy() for v in val_acc])\n",
        "np.save('/content/gdrive/My Drive/cs330_hw2/prob3_val_acc.npy', val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Setting parallel_iterations > 1 has no effect when executing eagerly. Consider calling map_fn with tf.function to execute fn in parallel.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Iteration 10: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 20: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.21600\n",
            "Iteration 30: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.18400\n",
            "Iteration 40: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.18400\n",
            "Iteration 50: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.21600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.20800\n",
            "Iteration 60: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20800\n",
            "Iteration 70: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 80: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.21600\n",
            "Iteration 90: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.23200\n",
            "Iteration 100: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.22400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.23200\n",
            "Iteration 110: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 120: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.20000\n",
            "Iteration 130: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 140: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 150: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.30400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.34400\n",
            "Iteration 160: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 170: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 180: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.26400\n",
            "Iteration 190: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.25600\n",
            "Iteration 200: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.32800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.27200\n",
            "Iteration 210: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 220: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 230: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 240: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 250: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.26400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 260: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.28000\n",
            "Iteration 270: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 280: pre-inner-loop train accuracy: 0.28000, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 290: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 300: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.32000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 310: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 320: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 330: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 340: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.32800\n",
            "Iteration 350: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.31200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 360: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 370: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 380: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 390: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 400: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.35200\n",
            "Iteration 410: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 420: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 430: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 440: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 450: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.38400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 460: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 470: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 480: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 490: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 500: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.35200\n",
            "Iteration 510: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 520: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 530: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 540: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 550: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.39200\n",
            "Iteration 560: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 570: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 580: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 590: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 600: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.36000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.37600\n",
            "Iteration 610: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 620: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 630: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 640: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 650: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 660: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 670: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 680: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 690: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 700: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.33600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.42400\n",
            "Iteration 710: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 720: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 730: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 740: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 750: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.37600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.40000\n",
            "Iteration 760: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 770: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 780: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 790: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 800: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 810: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 820: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 830: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 840: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 850: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.39200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.15200, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 860: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 870: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 880: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.41600\n",
            "Iteration 890: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 900: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 910: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.40800\n",
            "Iteration 920: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 930: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 940: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 950: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.41600\n",
            "Iteration 960: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 970: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 980: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 990: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.39200\n",
            "Iteration 1000: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1010: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1020: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1030: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1040: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1050: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.33600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1060: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1070: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1080: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1090: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1100: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1110: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1120: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1130: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1140: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1150: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.41600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1160: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1170: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1180: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 1190: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1200: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1210: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1220: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1230: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1240: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 1250: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.49600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1260: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1270: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1280: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1290: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 1300: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.25600, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1310: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1320: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1330: pre-inner-loop train accuracy: 0.28800, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1340: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1350: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1360: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1370: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1380: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1390: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 1400: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.48000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.43200\n",
            "Iteration 1410: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1420: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1430: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1440: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1450: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.49600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1460: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.44000\n",
            "Iteration 1470: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1480: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1490: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1500: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1510: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1520: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1530: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1540: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1550: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1560: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1570: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1580: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1590: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1600: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1610: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1620: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1630: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1640: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1650: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.27200, meta-validation post-inner-loop test accuracy: 0.45600\n",
            "Iteration 1660: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1670: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1680: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 1690: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.49600\n",
            "Iteration 1700: pre-inner-loop train accuracy: 0.28000, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1710: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1720: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1730: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 1740: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1750: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24000, meta-validation post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1760: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1770: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1780: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1790: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1800: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.52800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1810: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1820: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1830: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1840: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 1850: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.49600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 1860: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1870: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1880: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.61600\n",
            "Iteration 1890: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1900: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1910: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 1920: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 1930: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 1940: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.60000\n",
            "Iteration 1950: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.45600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 1960: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 1970: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 1980: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 1990: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2000: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.54400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2010: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2020: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2030: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2040: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2050: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.12800, meta-validation post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2060: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2070: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2080: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2090: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2100: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.47200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2110: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.59200\n",
            "Iteration 2120: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2130: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2140: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2150: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.49600\n",
            "Iteration 2160: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2170: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2180: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2190: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2200: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.57600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2210: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2220: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.59200\n",
            "Iteration 2230: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.48800\n",
            "Iteration 2240: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2250: pre-inner-loop train accuracy: 0.28800, post-inner-loop test accuracy: 0.46400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2260: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.59200\n",
            "Iteration 2270: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2280: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2290: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 2300: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.57600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.23200, meta-validation post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2310: pre-inner-loop train accuracy: 0.32000, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2320: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2330: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2340: pre-inner-loop train accuracy: 0.28000, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2350: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.61600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.26400, meta-validation post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2360: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2370: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2380: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2390: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2400: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.64000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2410: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2420: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2430: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2440: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2450: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.59200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24000, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2460: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 2470: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2480: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 2490: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2500: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.59200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2510: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 2520: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 2530: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2540: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2550: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.51200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2560: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2570: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2580: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2590: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2600: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.57600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.60000\n",
            "Iteration 2610: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2620: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 2630: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.48000\n",
            "Iteration 2640: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 2650: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.56800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2660: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2670: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 2680: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2690: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2700: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.52800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2710: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 2720: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2730: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2740: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2750: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.55200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24800, meta-validation post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2760: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 2770: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.59200\n",
            "Iteration 2780: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2790: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2800: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2810: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2820: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2830: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2840: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 2850: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.55200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2860: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.67200\n",
            "Iteration 2870: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 2880: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 2890: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 2900: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.53600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.60000\n",
            "Iteration 2910: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 2920: pre-inner-loop train accuracy: 0.12000, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 2930: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 2940: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 2950: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.44000\n",
            "Iteration 2960: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.56800\n",
            "Iteration 2970: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.58400\n",
            "Iteration 2980: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 2990: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 3000: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.55200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.27200, meta-validation post-inner-loop test accuracy: 0.56800\n",
            "Iteration 3010: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 3020: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 3030: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 3040: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 3050: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.52000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.64000\n",
            "Iteration 3060: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 3070: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.52800\n",
            "Iteration 3080: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.53600\n",
            "Iteration 3090: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.60800\n",
            "Iteration 3100: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.58400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.59200\n",
            "Iteration 3110: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 3120: pre-inner-loop train accuracy: 0.28800, post-inner-loop test accuracy: 0.54400\n",
            "Iteration 3130: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 3140: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.52000\n",
            "Iteration 3150: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.56000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.17600, meta-validation post-inner-loop test accuracy: 0.50400\n",
            "Iteration 3160: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.55200\n",
            "Iteration 3170: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.57600\n",
            "Iteration 3180: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.56800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkF_-yqT5rSa"
      },
      "source": [
        "test_K = [4, 6, 8, 10]\n",
        "for k in test_K:\n",
        "  print(\"meta test K={:d}\".format(k))\n",
        "  # Test protonet\n",
        "  proto_val_acc_k = run_protonet('./omniglot_resized/', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5, k_meta_test_shot=k, n_meta_test_query=4)\n",
        "  np.save('/content/gdrive/My Drive/cs330_hw2/prob3_proto_val_acc_test_K={:d}.npy'.format(k), proto_val_acc_k)\n",
        "  # Test MAML\n",
        "  maml_test_acc_k = run_maml(n_way=5, k_shot=k, inner_update_lr=0.4, num_inner_updates=1, meta_train=False, meta_test_set=True, meta_train_k_shot=1, \n",
        "                             model_file='/content/gdrive/My Drive/cs330_hw2/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False')\n",
        "  np.save('/content/gdrive/My Drive/cs330_hw2/prob3_maml_test_acc_test_K={:d}.npy'.format(k), maml_test_acc_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHQ-rsXk4UMb"
      },
      "source": [
        "plt.close('all')\n",
        "\n",
        "maml_acc_mean = np.empty(len(test_K))\n",
        "maml_acc_std = np.empty(len(test_K))\n",
        "for k in test_K:\n",
        "  maml_test_acc_k = np.load('/content/gdrive/My Drive/cs330_hw2/prob3_maml_test_acc_test_K={:d}.npy'.format(k))\n",
        "  maml_acc_mean[k] = np.mean(maml_test_acc_k) \n",
        "  maml_acc_std[k] = np.std(maml_test_acc_k)\n",
        "\n",
        "proto_acc_mean = np.array([])\n",
        "proto_acc_std = np.array([])\n",
        "# Errorbar version\n",
        "plt.errorbar(test_K, maml_acc_mean, yerr=maml_acc_std, color='k', label='ProtoNet')\n",
        "plt.errorbar(test_K, proto_acc_mean, yerr=proto_acc_std, color='tab:orange', label='MAML')\n",
        "# Errorband version\n",
        "plt.plot(test_K, proto_acc_mean, color='k', label='ProtoNet')\n",
        "plt.fill_between(test_K, proto_acc_mean - proto_acc_std, proto_acc_mean + proto_acc_std, alpha=0.2, facecolor='tab:gray')\n",
        "plt.plot(test_K, maml_acc_mean, color='k', label='MAML')\n",
        "plt.fill_between(test_K, maml_acc_mean - maml_acc_std, maml_acc_mean + maml_acc_std, alpha=0.2, facecolor='tab:orange')\n",
        "\n",
        "plt.ylabel('Meta-test accuracy')\n",
        "plt.xlabel('Meta-test K')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}